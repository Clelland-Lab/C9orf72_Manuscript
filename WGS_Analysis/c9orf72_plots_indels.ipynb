{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac7e9ad-97e3-487b-aa87-18ba6f781642",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pysam\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import fisher_exact\n",
    "import scipy.stats as stats\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75884aa6-b5f2-4274-af3e-0820dce14abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add the CHANGE seq beds\n",
    "alt7_taylor = 'alt7_taylor.bed'\n",
    "rex4_taylor = 'rex4_taylor.bed'\n",
    "rexc_taylor = 'rexc_taylor.bed'\n",
    "altc_taylor = 'altc_taylor.bed'\n",
    "\n",
    "#the cas offinder beds \n",
    "alt7_cof =  Path('small_bams_again') / 'beds' / 'casoffinder_beds' / 'casoffinder_alt7_4mismatches.bed'\n",
    "rex4_cof =  Path('small_bams_again') / 'beds' / 'casoffinder_beds' / 'casoffinder_rex4_4mismatches.bed'\n",
    "rexc_cof =  Path('small_bams_again') / 'beds' / 'casoffinder_beds' / 'casoffinder_rexc_4mismatches.bed'\n",
    "altc_cof =  Path('small_bams_again') / 'beds' / 'casoffinder_beds' / 'casoffinder_altc_4mismatches.bed'\n",
    "\n",
    "\n",
    "altc = Path('small_bams_again') / 'beds' / 'altc_lifted.bed'\n",
    "alt7 = Path('small_bams_again') / 'beds' / 'alt7_lifted.bed'\n",
    "rex4 = Path('small_bams_again') / 'beds' / 'rex4_lifted.bed'\n",
    "rexc = Path('small_bams_again') / 'beds' / 'rexc_lifted.bed'\n",
    "\n",
    "altc = 'altc_taylor.bed'\n",
    "alt7 = 'alt7_taylor.bed'\n",
    "rex4 = 'rex4_taylor.bed'\n",
    "rexc = 'rexc_taylor.bed'\n",
    "\n",
    "#read in the tsvs\n",
    "all_tsvs = {'ALTC':pd.read_csv(altc, sep='\\t',  names=['chrom','start','end','na','na2','REGION']),  \n",
    "    'ALT7':pd.read_csv(alt7, sep='\\t',  names=['chrom','start','end','na','na2','REGION']),\n",
    "                'REX4':pd.read_csv(rex4, sep='\\t',  names=['chrom','start','end','na','na2','REGION']),\n",
    "                'REXC':pd.read_csv(rexc, sep='\\t',  names=['chrom','start','end','na','na2','REGION'])}\n",
    "\n",
    "\n",
    "all_tsvs_cof = {'ALTC':pd.read_csv(altc_cof, sep='\\t',  names=['chrom','start','end','na','na2','REGION']),  \n",
    "    'ALT7':pd.read_csv(alt7_cof, sep='\\t',  names=['chrom','start','end','na','na2','REGION']),\n",
    "                'REX4':pd.read_csv(rex4_cof, sep='\\t',  names=['chrom','start','end','na','na2','REGION']),\n",
    "                'REXC':pd.read_csv(rexc_cof, sep='\\t',  names=['chrom','start','end','na','na2','REGION'])}\n",
    "\n",
    "\n",
    "\n",
    "edited_dfs = {'ALTC': [None, None],'ALT7': [None, None],\n",
    "      'REXC': [None, None],\n",
    "      'REX4':[None, None]}\n",
    "\n",
    "comp_dfs = {'ALTC': [None, None], \n",
    "            'ALT7': [None, None],\n",
    "      'REXC': [None, None],\n",
    "      'REX4':[None, None]}\n",
    "\n",
    "control_dfs = {'ALTC': [None, None],\n",
    "               'ALT7': None,\n",
    "      'REXC': None,\n",
    "      'REX4': None}\n",
    "\n",
    "edited_dfs_cof = {'ALTC': [None, None],'ALT7': [None, None],\n",
    "      'REXC': [None, None],\n",
    "      'REX4':[None, None]}\n",
    "\n",
    "comp_dfs_cof = {'ALTC': [None, None], \n",
    "            'ALT7': [None, None],\n",
    "      'REXC': [None, None],\n",
    "      'REX4':[None, None]}\n",
    "\n",
    "control_dfs_cof = {'ALTC': [None, None],\n",
    "               'ALT7': None,\n",
    "      'REXC': None,\n",
    "      'REX4': None}\n",
    "\n",
    "\n",
    "#these are the cassofinder csvs\n",
    "for condition in all_tsvs:\n",
    "    for replicate in  np.arange(0,2):\n",
    "        file_path = f\"{condition}again_casoffinder_new_concatanted_taylor_consensus_indel_rates_hg38{replicate}.csv\"  # Construct file name\n",
    "        edited_dfs_cof[condition][replicate] = pd.read_csv(file_path)\n",
    "        edited_dfs_cof[condition][replicate]['chrom_orig'] = all_tsvs_cof[condition]['chrom'].to_list()\n",
    "        edited_dfs_cof[condition][replicate]['start_orig'] = all_tsvs_cof[condition]['start'].to_list()\n",
    "        edited_dfs_cof[condition][replicate]['end_orig'] = all_tsvs_cof[condition]['end'].to_list()\n",
    "        \n",
    "for condition in all_tsvs:\n",
    "    file_path = f\"{condition}again_casoffinder_new_concatanted_taylor_consensus_indel_rates_control_hg38.csv\"  # Using f-string for readability\n",
    "    control_dfs_cof[condition] = pd.read_csv(file_path)\n",
    "    control_dfs_cof[condition]['chrom_orig'] = all_tsvs_cof[condition]['chrom'].to_list()\n",
    "    control_dfs_cof[condition]['start_orig'] = all_tsvs_cof[condition]['start'].to_list()\n",
    "    control_dfs_cof[condition]['end_orig'] = all_tsvs_cof[condition]['end'].to_list()\n",
    "\n",
    "for condition in all_tsvs:\n",
    "    for replicate in np.arange(0,2):\n",
    "        file_path = f\"casoffinder_{condition}{replicate}_taylor_consensus_comparison_hg38.csv\"  # Using f-string for readability\n",
    "        comp_dfs_cof[condition][replicate] = pd.read_csv(file_path)\n",
    "        comp_dfs_cof[condition][replicate]['chrom_orig'] = all_tsvs_cof[condition]['chrom'].to_list()\n",
    "        comp_dfs_cof[condition][replicate]['start_orig'] = all_tsvs_cof[condition]['start'].to_list()\n",
    "        comp_dfs_cof[condition][replicate]['end_orig'] = all_tsvs_cof[condition]['end'].to_list()\n",
    "\n",
    "#these are the normal ones\n",
    "for condition in all_tsvs:\n",
    "    for replicate in  np.arange(0,2):\n",
    "        file_path = f\"{condition}again_new_concatanted_taylor_consensus_indel_rates_hg38{replicate}.csv\"  # Construct file name\n",
    "        edited_dfs[condition][replicate] = pd.read_csv(file_path)\n",
    "        edited_dfs[condition][replicate]['chrom_orig'] = all_tsvs[condition]['chrom'].to_list()\n",
    "        edited_dfs[condition][replicate]['start_orig'] = all_tsvs[condition]['start'].to_list()\n",
    "        edited_dfs[condition][replicate]['end_orig'] = all_tsvs[condition]['end'].to_list()\n",
    "\n",
    "for condition in all_tsvs:\n",
    "    file_path = f\"{condition}again_new_concatanted_taylor_consensus_indel_rates_control_hg38.csv\"  # Using f-string for readability\n",
    "    control_dfs[condition] = pd.read_csv(file_path)\n",
    "    control_dfs[condition]['chrom_orig'] = all_tsvs[condition]['chrom'].to_list()\n",
    "    control_dfs[condition]['start_orig'] = all_tsvs[condition]['start'].to_list()\n",
    "    control_dfs[condition]['end_orig'] = all_tsvs[condition]['end'].to_list()\n",
    "\n",
    "for condition in all_tsvs:\n",
    "    for replicate in np.arange(0,2):\n",
    "        file_path = f\"change_seq_{condition}{replicate}_taylor_consensus_comparison_hg38.csv\"  # Using f-string for readability\n",
    "        print(file_path)\n",
    "        comp_dfs[condition][replicate] = pd.read_csv(file_path)\n",
    "        comp_dfs[condition][replicate]['chrom_orig'] = all_tsvs[condition]['chrom'].to_list()\n",
    "        comp_dfs[condition][replicate]['start_orig'] = all_tsvs[condition]['start'].to_list()\n",
    "        comp_dfs[condition][replicate]['end_orig'] = all_tsvs[condition]['end'].to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc7c14a-02bb-4c03-b83a-7db382b20734",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import combine_pvalues\n",
    "comp_dfs['ALTC'][0]\n",
    "\n",
    "#combine the graphs\n",
    "for condition in all_tsvs.keys():\n",
    "    print(comp_dfs[condition][0].columns)\n",
    "    comp_dfs_cof[condition][0]['total wt reads rep2'] = comp_dfs_cof[condition][1]['total wt reads'] \n",
    "    comp_dfs_cof[condition][0]['total edited reads rep2'] = comp_dfs_cof[condition][1]['total edited reads'] \n",
    "    comp_dfs_cof[condition][0]['edited reads with indels rep2'] = comp_dfs_cof[condition][1]['edited reads with indels '] \n",
    "    comp_dfs_cof[condition][0]['wt reads with indels rep2'] = comp_dfs_cof[condition][1]['wt reads with indels'] \n",
    "    comp_dfs_cof[condition][0]['edited indel % rep2'] = comp_dfs_cof[condition][1]['edited indel %']\n",
    "    comp_dfs_cof[condition][0]['wt indel % rep2'] = comp_dfs_cof[condition][1]['wt indel %'] \n",
    "    comp_dfs_cof[condition][0]['indel p-value rep2'] = comp_dfs_cof[condition][1]['indel p-value'] \n",
    "\n",
    "    #use combine_pvalues to combine two p-values that result from a fisher test\n",
    "    combined_pvals = []\n",
    "    for p1, p2 in zip(comp_dfs_cof[condition][0]['indel p-value'], comp_dfs_cof[condition][0]['indel p-value rep2']):\n",
    "        stat, p_combined = combine_pvalues([p1, p2], method='fisher')\n",
    "        combined_pvals.append(p_combined)\n",
    "\n",
    "    #combine the p-values \n",
    "    comp_dfs_cof[condition][0]['fishers combined indel p-value'] = combined_pvals\n",
    "\n",
    "    comp_dfs_cof[condition][0]['combined_edited_reads'] = comp_dfs_cof[condition][0]['total edited reads'] + comp_dfs_cof[condition][1]['total edited reads']\n",
    "    comp_dfs_cof[condition][0]['combined_wt_reads'] = comp_dfs_cof[condition][0]['total wt reads'] + comp_dfs_cof[condition][1]['total wt reads']\n",
    "    comp_dfs_cof[condition][0]['combined_edited_reads_w_indels'] = comp_dfs_cof[condition][0]['edited reads with indels '] + comp_dfs_cof[condition][1]['edited reads with indels ']\n",
    "    comp_dfs_cof[condition][0]['combined_wt_reads_w_indels'] = comp_dfs_cof[condition][0]['wt reads with indels'] + comp_dfs_cof[condition][1]['wt reads with indels']\n",
    "    comp_dfs_cof[condition][0]['combined edited indel %'] = comp_dfs_cof[condition][0]['combined_edited_reads_w_indels'] / comp_dfs_cof[condition][0]['combined_edited_reads']\n",
    "    comp_dfs_cof[condition][0]['combined wt indel %'] = comp_dfs_cof[condition][0]['combined_wt_reads_w_indels'] / comp_dfs_cof[condition][0]['combined_wt_reads']\n",
    "\n",
    "    #a different way of combining based off total reads - not used in final csv table\n",
    "    comp_dfs_cof[condition][0]['fishers combined indel %'] = comp_dfs_cof[condition][0]['combined_wt_reads_w_indels'] / comp_dfs_cof[condition][0]['combined_wt_reads']\n",
    "\n",
    "\n",
    "    #iterate through the rows in the comparison table and preform the fishers exact test on the combined data (not used in final csv)\n",
    "    for index, row in comp_dfs_cof[condition][0].iterrows():\n",
    "        region = row['REGION']\n",
    "        cont = pd.DataFrame([])\n",
    "        cont_indel = pd.DataFrame([])\n",
    "        cont['wt'] = [int(row['combined_wt_reads_w_indels']),\n",
    "                      int(row['combined_wt_reads']) - int(row['combined_wt_reads_w_indels'])]\n",
    "        \n",
    "        cont['edited'] = [int(row['combined_edited_reads_w_indels']),\n",
    "                      int(row['combined_edited_reads']) - int(row['combined_edited_reads_w_indels'])]\n",
    "\n",
    "        cont['type'] = ['indel', 'no indel']\n",
    "        cont.set_index('type', inplace=True)\n",
    "        odds_ratio, p_value = stats.fisher_exact(cont)\n",
    "        comp_dfs_cof[condition][0].loc[comp_dfs_cof[condition][0]['REGION'] == region, 'combined indel p-value'] = p_value\n",
    "\n",
    "\n",
    "#recreating the p-value calculation now that we include all locations\n",
    "for condition in all_tsvs.keys():   \n",
    "    comp_dfs[condition][0]['total wt reads rep2'] = comp_dfs[condition][1]['total wt reads'] \n",
    "    comp_dfs[condition][0]['total edited reads rep2'] = comp_dfs[condition][1]['total edited reads'] \n",
    "    comp_dfs[condition][0]['edited reads with indels rep2'] = comp_dfs[condition][1]['edited reads with indels '] \n",
    "    comp_dfs[condition][0]['wt reads with indels rep2'] = comp_dfs[condition][1]['wt reads with indels'] \n",
    "    comp_dfs[condition][0]['edited indel % rep2'] = comp_dfs[condition][1]['edited indel %'] \n",
    "    comp_dfs[condition][0]['indel p-value rep2'] = comp_dfs[condition][1]['indel p-value'] \n",
    "    comp_dfs[condition][0]['wt indel % rep2'] = comp_dfs[condition][1]['wt indel %'] \n",
    "\n",
    "    comp_dfs[condition][0]['combined_edited_reads'] = comp_dfs[condition][0]['total edited reads'] + comp_dfs[condition][1]['total edited reads']\n",
    "    comp_dfs[condition][0]['combined_wt_reads'] = comp_dfs[condition][0]['total wt reads'] + comp_dfs[condition][1]['total wt reads']\n",
    "    comp_dfs[condition][0]['combined_edited_reads_w_indels'] = comp_dfs[condition][0]['edited reads with indels '] + comp_dfs[condition][1]['edited reads with indels ']\n",
    "    comp_dfs[condition][0]['combined_wt_reads_w_indels'] = comp_dfs[condition][0]['wt reads with indels'] + comp_dfs[condition][1]['wt reads with indels']\n",
    "    comp_dfs[condition][0]['combined edited indel %'] = comp_dfs[condition][0]['combined_edited_reads_w_indels'] / comp_dfs[condition][0]['combined_edited_reads']\n",
    "    comp_dfs[condition][0]['combined wt indel %'] = comp_dfs[condition][0]['combined_wt_reads_w_indels'] / comp_dfs[condition][0]['combined_wt_reads']\n",
    "\n",
    "    combined_pvals = []\n",
    "    for p1, p2 in zip(comp_dfs[condition][0]['indel p-value'], comp_dfs[condition][0]['indel p-value rep2']):\n",
    "        stat, p_combined = combine_pvalues([p1, p2], method='fisher')\n",
    "        combined_pvals.append(p_combined)\n",
    "        \n",
    "    comp_dfs[condition][0]['fishers combined indel p-value'] = combined_pvals\n",
    "    \n",
    "    #calculates the p-value for each of the rows\n",
    "    for index, row in comp_dfs[condition][0].iterrows():\n",
    "        region = row['REGION']\n",
    "        cont = pd.DataFrame([])\n",
    "        cont_indel = pd.DataFrame([])\n",
    "        cont['wt'] = [int(row['combined_wt_reads_w_indels']),\n",
    "                      int(row['combined_wt_reads']) - int(row['combined_wt_reads_w_indels'])]\n",
    "        \n",
    "        cont['edited'] = [int(row['combined_edited_reads_w_indels']),\n",
    "                      int(row['combined_edited_reads']) - int(row['combined_edited_reads_w_indels'])]\n",
    "\n",
    "        cont['type'] = ['indel', 'no indel']\n",
    "        cont.set_index('type', inplace=True)\n",
    "        odds_ratio, p_value = stats.fisher_exact(cont)\n",
    "        comp_dfs[condition][0].loc[comp_dfs[condition][0]['REGION'] == region, 'combined indel p-value'] = p_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e21ece-7cbe-4bea-8b98-72acb94ee316",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add a row for the new table\n",
    "def construct_comp_tbl(wt, edited):\n",
    "    comp_df = pd.DataFrame([])\n",
    "\n",
    "    # Ensure 'REGION' column is included\n",
    "    comp_df['REGION'] = wt['REGION'].tolist()\n",
    "    \n",
    "    #construct a contigency table\n",
    "    for index, row in wt.iterrows():\n",
    "        cont = pd.DataFrame([])\n",
    "        cont_indel = pd.DataFrame([])\n",
    "        \n",
    "        region = row['REGION']\n",
    "        cont['wt'] = [int(row['combined_wt_reads_w_indels']),\n",
    "        int(row['combined_wt_reads']) - int(row['combined_wt_reads_w_indels'])]\n",
    "\n",
    "        row2 = edited.iloc[index]\n",
    "        cont['edited'] = [int(row2['combined_edited_reads_w_indels']),\n",
    "        int(row2['combined_edited_reads']) - int(row2['combined_edited_reads_w_indels'])]\n",
    "        \n",
    "        cont['type'] = ['indel', 'no indel']\n",
    "        cont.set_index('type', inplace=True)\n",
    "        odds_ratio, p_value = stats.fisher_exact(cont)\n",
    "\n",
    "        comp_df.loc[comp_df['REGION'] == region, 'chrom'] = row['chrom']\n",
    "        comp_df.loc[comp_df['REGION'] == region, 'start'] = row2['start']\n",
    "        comp_df.loc[comp_df['REGION'] == region, 'end'] = row['end']\n",
    "        comp_df.loc[comp_df['REGION'] == region, 'combined indel p-value'] = p_value\n",
    "        \n",
    "    return comp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790ee24f-f196-4172-b3d1-4bc109cde0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#understand the unique positions\n",
    "for condition in comp_dfs:\n",
    "    for replicate in np.arange(0,1):\n",
    "        print(comp_dfs[condition][replicate]['chrom'].unique())\n",
    "\n",
    "for condition in comp_dfs_cof:\n",
    "    for replicate in np.arange(0,1):\n",
    "        print(comp_dfs_cof[condition][replicate]['chrom'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fcd95f-622c-48bc-b5e5-40b1b6a71ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "#combine the two tables\n",
    "for condition in all_tsvs:\n",
    "    for replicate in range(1):  # Iterate through two replicates (0 and 1)\n",
    "        comp_dfs_cof[condition][replicate]['category'] = 'Cas-OFFinder'\n",
    "        comp_dfs[condition][replicate]['category'] = 'CHANGE-seq'\n",
    "        comp_dfs[condition][replicate]=pd.concat([comp_dfs_cof[condition][replicate],comp_dfs[condition][replicate]])\n",
    "\n",
    "\n",
    "#aggregate the \n",
    "for condition in ['ALT7', 'ALTC', 'REXC', 'REX4']:\n",
    "    for replicate in [0, 1]:\n",
    "        df = comp_dfs[condition][replicate]\n",
    "\n",
    "        agg_dict = {}\n",
    "        for col in df.columns:\n",
    "            if col == 'start':\n",
    "                continue  # grouping column\n",
    "            elif col == 'category':\n",
    "                # Join all category values (as strings)\n",
    "                agg_dict[col] = lambda x: ' and '.join(x.astype(str))\n",
    "            else:\n",
    "                # Take the first value for any other column\n",
    "                agg_dict[col] = 'first'\n",
    "\n",
    "        comp_dfs[condition][replicate] = df.groupby('start').agg(agg_dict).reset_index()\n",
    "\n",
    "for condition in all_tsvs:\n",
    "    for replicate in range(0,1):  \n",
    "        # Get the raw p-values for the current replicate and condition\n",
    "        raw_p_values = comp_dfs[condition][replicate]['indel p-value']\n",
    "        combined_raw_p_values = comp_dfs[condition][replicate]['combined indel p-value']\n",
    "\n",
    "        # Combine based off fishers combined p-value calculation\n",
    "        fishers_combined_raw_p_values = comp_dfs[condition][replicate]['fishers combined indel p-value']\n",
    "        \n",
    "        # apply Benjamini-Hochberg correction (FDR control) to the fishers combined p-value\n",
    "        reject, fisher_adjusted_p_values, _, _ = multipletests(fishers_combined_raw_p_values, alpha=0.05, method='fdr_bh')\n",
    "        \n",
    "        # Apply Benjamini-Hochberg correction (FDR control)\n",
    "        reject, adjusted_p_values, _, _ = multipletests(combined_raw_p_values, alpha=0.05, method='fdr_bh')\n",
    "        combined_reject, combined_adjusted_p_values, combined_, combined_ = multipletests(combined_raw_p_values, alpha=0.05, method='fdr_bh')\n",
    "        \n",
    "        # Add the adjusted p-values back into the DataFrame (not used)\n",
    "        comp_dfs[condition][replicate]['adjusted_raw_p_values'] = adjusted_p_values\n",
    "        comp_dfs[condition][replicate]['combined adjusted indel p-value'] = combined_adjusted_p_values\n",
    "\n",
    "        #final p-value metric to use\n",
    "        comp_dfs[condition][replicate]['fishers p-values adjusted with benjamini-hochberg'] = fisher_adjusted_p_values\n",
    "\n",
    "\n",
    "for condition in ['ALT7', 'ALTC', 'REXC', 'REX4']:\n",
    "    for replicate in [0, 1]:\n",
    "        comp_dfs[condition][replicate].to_csv(f\"{condition}{replicate}_combined_original_new.csv\", sep='\\t', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5017fef-aeb7-4b45-9ae2-32ca1ce3c2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_dfs['ALTC'][0]['category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55abc90f-774f-4cc7-9ab9-20dfeda62f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "min(comp_dfs['ALTC'][0]['fishers combined indel p-value']) #correct\n",
    "min(multipletests(comp_dfs['ALTC'][0]['fishers combined indel p-value'], alpha=0.05, method='fdr_bh')[1]) #correct\n",
    "min(comp_dfs['ALTC'][0]['fishers p-values adjusted with benjamini-hochberg'].to_list()) #not correct\n",
    "\n",
    "\n",
    "fishers_combined_raw_p_values = comp_dfs['ALTC'][0]['fishers combined indel p-value']\n",
    "min(fishers_combined_raw_p_values) #correct\n",
    "reject, fisher_adjusted_p_values, _, _ = multipletests(fishers_combined_raw_p_values, alpha=0.05, method='fdr_bh')\n",
    "min(fisher_adjusted_p_values) #correct\n",
    "min(comp_dfs['ALTC'][0]['fishers combined indel p-value']) #correct\n",
    "min(comp_dfs['ALTC'][0]['fishers combined indel p-value']) #correct\n",
    "min(comp_dfs['ALTC'][0]['fishers p-values adjusted with benjamini-hochberg'].to_list()) #not correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bd8259-2cc0-48d2-9d41-b6130cfda32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the chrom_order list - removing chromosome Y for this analysis as the c9p5 cell line does not contain chrom Y\n",
    "chrom_order = ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', \n",
    "               'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', \n",
    "               'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX']\n",
    "\n",
    "# Loop through each condition and replicate\n",
    "for condition in all_tsvs:\n",
    "    for replicate in np.arange(0, 2):\n",
    "        print(condition, replicate)\n",
    "        # Read the DataFrame (assuming file_path is defined earlier)\n",
    "        \n",
    "        # Filter the DataFrame to include only rows with 'chrom' values in chrom_order\n",
    "        filtered_df = comp_dfs[condition][replicate][comp_dfs[condition][replicate]['chrom'].isin(chrom_order)]\n",
    "        \n",
    "        # Sort the DataFrame by 'chrom' according to the chrom_order list\n",
    "        filtered_df['chrom'] = pd.Categorical(filtered_df['chrom'], categories=chrom_order, ordered=True)\n",
    "        sorted_df = filtered_df.sort_values(by=['chrom', 'start'])\n",
    "        \n",
    "        # Update the DataFrame with the filtered and sorted data\n",
    "        comp_dfs[condition][replicate] = sorted_df\n",
    "\n",
    "        # If you want to display the sorted DataFrame\n",
    "        display(sorted_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58509e1-e2a3-4acb-8e1a-414fcc158658",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Define alternating black-gray colors\n",
    "black_gray = ['#000000', '#808080']  # Black and Gray\n",
    "\n",
    "# # Generate dictionary with alternating colors\n",
    "# chromosomes = [f'chr{i}' for i in range(1, 23)]\n",
    "# custom_palette = {chrom: black_gray[i % 2] for i, chrom in enumerate(chromosomes)}\n",
    "# custom_palette['chrX'] = '#000000'\n",
    "# custom_palette['chrY'] = '#808080'\n",
    "# #color based off location\n",
    "\n",
    "\n",
    "#color based of  what its located in\n",
    "category = ['CHANGE-seq', 'Cas-OFFinder and CHANGE-seq', 'Cas-OFFinder']\n",
    "ordered_labels = ['CHANGE-seq', 'Cas-OFFinder', 'Cas-OFFinder and CHANGE-seq']\n",
    "desired_order = ['CHANGE-seq', 'Cas-OFFinder and CHANGE-seq', 'Cas-OFFinder']\n",
    "custom_palette = {'CHANGE-seq': '#1b9e77',  'Cas-OFFinder': '#d95f02', 'Cas-OFFinder and CHANGE-seq': '#46436b'}\n",
    "\n",
    "# Print to verify\n",
    "print(custom_palette)\n",
    "\n",
    "\n",
    "#need to modify to include the chromosome X and the chromsome Y\n",
    "#make the y axis the adjusted p-value instead\n",
    "\n",
    "count=0\n",
    "for condition in all_tsvs:\n",
    "    for replicate in np.arange(0,1):\n",
    "        # Create the data - select the columns of interest\n",
    "        data = {\n",
    "            'chrom': comp_dfs[condition][replicate]['chrom_orig'],\n",
    "            'category': comp_dfs[condition][replicate]['category'],\n",
    "            'start': comp_dfs[condition][replicate]['start_orig'],\n",
    "            'end': comp_dfs[condition][replicate]['end_orig'],\n",
    "            'p-value': comp_dfs[condition][replicate]['fishers p-values adjusted with benjamini-hochberg']\n",
    "        }\n",
    "        \n",
    "        df = pd.DataFrame(data)\n",
    "        df['%edited difference'] = comp_dfs[condition][replicate]['edited indel %'] - comp_dfs[condition][replicate]['wt indel %']\n",
    "\n",
    "        start_offset = []\n",
    "\n",
    "        # Initialize the dictionary for offsets\n",
    "        offsets = {}\n",
    "        \n",
    "        # Start by initializing the current position\n",
    "        current_pos = 0\n",
    "        \n",
    "        # Iterate through each unique chromosome\n",
    "        for chrom in df['chrom'].unique():            \n",
    "            # Get the data for the current chromosome\n",
    "            chrom_data = df[df['chrom'] == chrom]\n",
    "            \n",
    "            # Find the maximum start position for the chromosome (end position)\n",
    "            chrom_end = chrom_data['start'].max()  # Max position = end position for this chromosome\n",
    "            \n",
    "            # Store the offset (end position of the chromosome) in the dictionary\n",
    "            offsets[chrom] = current_pos  # Offset is the cumulative position\n",
    "            \n",
    "            # Update the current position for the next chromosome\n",
    "            current_pos += chrom_end  # Add the current chromosome's end position to the cumulative position\n",
    "        \n",
    "        # Print the offsets dictionary to see the results\n",
    "        df['offset_start'] = df.apply(lambda row: row['start'] + offsets[row['chrom']], axis=1)\n",
    " \n",
    "        # Transform p-values to -log10(p-value)\n",
    "        df['-log10 p-value'] = -np.log10(df['p-value'])\n",
    "        \n",
    "        # Plot the Manhattan plot\n",
    "        plt.figure(figsize=(13, 8))\n",
    "        \n",
    "        # Create scatter plot and remove edgecolor (no outline for dots)\n",
    "        sns.scatterplot(data=df, x=\"offset_start\", y=\"-log10 p-value\", hue=\"category\", palette=custom_palette, alpha=0.8, edgecolor='none', s = 300)\n",
    "            \n",
    "        # Label the plot\n",
    "        plt.xlabel(\"Chromosome\", fontsize=30,  fontfamily='Arial')\n",
    "        plt.ylabel(\"–log10(p), Edited vs. Control Indel Rate\", fontsize=30,  fontfamily='Arial')\n",
    "        if condition == 'ALT7':\n",
    "            title_name = \"7'\"\n",
    "        if condition == 'ALTC':\n",
    "            title_name = \"C'\"\n",
    "        if condition == 'REXC':\n",
    "            title_name = \"C\"\n",
    "        if condition == 'REX4':\n",
    "            title_name = '4'\n",
    "        plt.title('On and Off-Targets detected by WGS for gRNA ' + title_name, fontsize=25,  fontfamily='Arial')\n",
    "        \n",
    "        # Customize the X-axis to display chromosomes properly (and adjust spacing if needed)\n",
    "        chrom_boundaries = []\n",
    "        chrom_labels = []\n",
    "        \n",
    "        # Track the cumulative position for each chromosome\n",
    "        current_pos = 0\n",
    "        for chrom in df['chrom'].unique():\n",
    "            chrom_data = df[df['chrom'] == chrom]\n",
    "            chrom_end = chrom_data['start'].max()  # Get the max position for the chromosome\n",
    "            chrom_boundaries.append(current_pos)  # Add the offset\n",
    "            chrom_labels.append(chrom)  # Add the chrom label\n",
    "            current_pos += chrom_end  # Update current position\n",
    "\n",
    "        #find which points are signficant\n",
    "        significant_df = df[df['p-value'] < 0.05]\n",
    "        \n",
    "        # Label significant points\n",
    "        for _, row in significant_df.iterrows():\n",
    "            if row['chrom'] == 'chr9':\n",
    "                plt.text(row['offset_start'] - 10, row['-log10 p-value'] + 10, \n",
    "                     f\"On-Target {row['chrom']}:{int(row['start'])}:{int(row['end'])}\", fontsize=25, ha='right', color='black',  fontfamily='Arial')\n",
    "            else:\n",
    "                 plt.text(row['offset_start'] - 10, row['-log10 p-value'] + 10, \n",
    "                     f\"Off-Target {row['chrom']}:{int(row['start'])}:{int(row['end'])}\", fontsize=25, ha='right', color='black',  fontfamily='Arial')\n",
    "\n",
    "        # Set custom x-ticks based on chromosome boundaries\n",
    "        plt.tick_params(axis='x', width=2)\n",
    "        plt.tick_params(axis='y', width=2)\n",
    "        plt.xticks(ticks=chrom_boundaries, labels=[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,'X'], rotation=90,fontsize=30)\n",
    "        plt.tick_params(axis='x', length=12)\n",
    "        plt.tick_params(axis='y', length=12)\n",
    "        plt.yticks(fontsize=30)\n",
    "\n",
    "        \n",
    "        # Add a legend to indicate the chromosomes\n",
    "        handles, labels = plt.gca().get_legend_handles_labels()\n",
    "        label_to_handle = dict(zip(labels, handles))\n",
    "        \n",
    "        ordered_labels = ['CHANGE-seq', 'Cas-OFFinder', 'Cas-OFFinder and CHANGE-seq']\n",
    "        ordered_handles = [label_to_handle[label] for label in ordered_labels if label in label_to_handle]\n",
    "        \n",
    "        legend = plt.legend(ordered_handles, ordered_labels, title=\"Nomination Method\", bbox_to_anchor=(0.63, 1), loc='upper left', fontsize='x-large')\n",
    "        legend.get_title().set_fontsize('x-large') \n",
    "\n",
    "        plt.ylim(-10,230)\n",
    "            \n",
    "        # Show the plot\n",
    "        count += 1\n",
    "        plt.axhline(y=3, color='blue', linestyle=':', linewidth=2)\n",
    "        plt.tight_layout()\n",
    "        # Increment the count\n",
    "        \n",
    "        # Save the plot with the count in the filename\n",
    "        plt.savefig(f\"Final_Plot_new_again{count}.svg\", format='svg')  # <-- Use f-string to evaluate count\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d5698c-7e3e-4f0f-8142-1bfa0fa628e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"filtered_tables\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "excel_path = os.path.join(output_dir, \"WGS_pvalues_fishers_2_replicates_result.xlsx\")\n",
    "\n",
    "\n",
    "#produce the final tables that contain only all the columns of interest\n",
    "cats = {'ALTC': 'grna_c_prime', 'ALT7':'grna_7_prime', 'REX4': 'grna_4', 'REXC':'grna_c'}\n",
    "with pd.ExcelWriter(excel_path, engine=\"openpyxl\") as writer:\n",
    "    for condition in all_tsvs:\n",
    "        for replicate in np.arange(0, 1):\n",
    "            df = comp_dfs[condition][replicate].copy()\n",
    "            \n",
    "            #irrelevant columns to drop for final table\n",
    "            cols_to_drop = ['adjusted indel p-value',\n",
    "                            'combined indel p-value', 'combined_edited_reads', 'combined_wt_reads',\n",
    "                            'combined edited indel %', 'combined wt indel %', \n",
    "                            'combined_edited_reads_w_indels', \n",
    "                            'combined_wt_reads_w_indels', \n",
    "                            'raw_p_values',  'Unnamed: 0', 'REGION',\n",
    "                            'combined adjusted indel p-value', 'adjusted_raw_p_values', \n",
    "                            'fishers combined indel %', 'chrom_orig', 'start_orig', 'end_orig', 'trial']\n",
    "            df.drop([c for c in cols_to_drop if c in df.columns], axis=1, inplace=True)\n",
    "\n",
    "            # Sheet name: e.g., \"cond1_rep0\"\n",
    "            sheet_name = f\"{cats[condition]}\"[:31]  # Excel limit = 31 chars\n",
    "            df.to_excel(writer, sheet_name=sheet_name, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f56046f-1d97-45cd-bff2-eb010e8a39ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
