{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e780026-c437-42b2-8329-2a7bc1d79643",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pysam\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06074eb5-6a37-402d-9b33-756694cf84a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#manually identified the inversion reads\n",
    "inversion_reads = ['m84127_240712_175808_s4/246483519/ccs',\n",
    "'m84127_240712_175808_s4/165154071/ccs',\n",
    "'m84127_240712_175808_s4/41227253/ccs',\n",
    "'m84127_240802_194716_s2/230297831/ccs',\n",
    "'m84127_240802_194716_s2/91030239/ccs',\n",
    "'m84127_240712_175808_s4/45553173/ccs',\n",
    "'m84127_240712_175808_s4/69734245/ccs',\n",
    "'m84127_240712_175808_s4/25760525/ccs',\n",
    "'m84127_240712_175808_s4/200609304/ccs',\n",
    "'m84127_240712_175808_s4/212337140/ccs',\n",
    "'m84127_240712_175808_s4/60817797/ccs',\n",
    "'m84127_240712_175808_s4/223613241/ccs',\n",
    "'m84127_240712_175808_s4/246221764/ccs',\n",
    "'m84127_240712_175808_s4/5771352/ccs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705fb7f8-19f0-440c-8bba-307ed407d491",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting the coordinates of the exonic regions in c9orf72\n",
    "ex1_coord = (27573431,27573481)\n",
    "ex2_coord = (27573709,27573866)\n",
    "between = (27573481, 27573709)\n",
    "\n",
    "#for the region between the start of exon1 and end of exon2 - extract the deletion and insertion coordinates\n",
    "#input:\n",
    "#     read\n",
    "#     l: location in the reference\n",
    "#     l_read: the location in the read where enters exon1 (not ness. same as ref loc)\n",
    "#     segment_index: which cigar tuple we are at\n",
    "\n",
    "the_read = None\n",
    "def check_exonic_overlaps(read, l, l_read, segment_index):\n",
    "    read_del_lst = []\n",
    "    read_ins_lst = []\n",
    "    for op, length in read.cigartuples[segment_index:]:\n",
    "        if l > ex2_coord[1]:\n",
    "            return read_del_lst, read_ins_lst\n",
    "        if op == 0:  \n",
    "            l_read += length\n",
    "            l += length\n",
    "        elif op == 1:\n",
    "            l_read += length\n",
    "            start = l\n",
    "            end = l + length\n",
    "            read_ins_lst.append((start, end))\n",
    "        elif op == 2:  # Deletion\n",
    "            start = l\n",
    "            end = l + length\n",
    "            l += length\n",
    "            read_del_lst.append((start, end))\n",
    "        elif op == 3:  # Skipped region\n",
    "            l += length #moving ahead in the reference\n",
    "\n",
    "        elif op == 4:  # Soft clipping\n",
    "            l_read += length\n",
    "            l += length \n",
    "        elif op == 5:  # Hard clipping \n",
    "            pass\n",
    "        elif op == 6:  # Padding\n",
    "            pass\n",
    "        elif op == 7:  # Segment equal \n",
    "            l_read += length\n",
    "            l += length\n",
    "        elif op == 8:\n",
    "            l_read += length\n",
    "            l += length\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown CIGAR operation code {op}\")\n",
    "\n",
    "    return read_del_lst, read_ins_lst\n",
    "\n",
    "def find_deletion_locs(inbam):\n",
    "    deletions = []\n",
    "    insertions = []\n",
    "    count = 0\n",
    "\n",
    "    for read in inbam.fetch(\"chr9\", 27557179, 27580459):\n",
    "        count += 1\n",
    "        \n",
    "        read_start = read.reference_start\n",
    "        read_end = read.reference_end\n",
    "        \n",
    "        if read_start < ex1_coord[0] or read.query_name in inversion_reads:\n",
    "            ref_start = read.reference_start  # Correct attribute for reference start position\n",
    "    \n",
    "            if read.cigartuples[0][0] == 4:\n",
    "                ref_start = ref_start - read.cigartuples[0][1]\n",
    "    \n",
    "            cigar = read.cigartuples\n",
    "    \n",
    "            #the bases traversed - where we are in the read\n",
    "            l_read = 0\n",
    "            #the exact start position of the read - where we are in the reference\n",
    "            l = ref_start\n",
    "    \n",
    "            segment_index = 0\n",
    "            for op, length in cigar:\n",
    "                #if we are at the exonic region in the read\n",
    "                if l + length > ex1_coord[0]:\n",
    "                    deletion_lst, insertion_lst = check_exonic_overlaps(read, l, l_read, segment_index)\n",
    "                    deletions.append(deletion_lst)\n",
    "                    insertions.append(insertion_lst)\n",
    "                    break\n",
    "    \n",
    "                if op == 0:  # NOT SURE WHAT THE MEANING OF THIS IS\n",
    "                    l_read += length\n",
    "                    l += length\n",
    "    \n",
    "                elif op == 1:  # Insertion (this is correct)\n",
    "                    l_read += length\n",
    "    \n",
    "                elif op == 2:  # Deletion (this is correct)\n",
    "                    l += length\n",
    "    \n",
    "                elif op == 3:  # Skipped region\n",
    "                    l += length #moving ahead in the reference\n",
    "    \n",
    "                elif op == 4:  # Soft clipping\n",
    "                    l_read += length\n",
    "                    l += length #\n",
    "    \n",
    "                elif op == 5:  # Hard clipping NOT SURE\n",
    "                    pass\n",
    "                elif op == 6:  # Padding NOT SURE\n",
    "                    pass\n",
    "                elif op == 7:  # Segment equal (this is correct)\n",
    "                    l_read += length\n",
    "                    l += length\n",
    "                elif op == 8:  # seems to be sequence mismatcj - they both advance then NOT SURE\n",
    "                    l_read += length\n",
    "                    l += length\n",
    "                else:\n",
    "                    print('here')\n",
    "                    raise ValueError(f\"Unknown CIGAR operation code {op}\")\n",
    "    \n",
    "                segment_index += 1\n",
    "    return deletions, insertions\n",
    "    # Close the BAM file\n",
    "    inbam.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6d961e-0268-4f04-bc0c-7b2f6c81170b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''IMPORTANT LOCATIONS IN THE FILE'''\n",
    "ex1_coord = (27573431-1,27573481)\n",
    "ex2_coord = (27573709-1,27573866)\n",
    "between = (27573481-1,27573709)\n",
    "\n",
    "c_loc = (27573507-1,27573527) \n",
    "g4_loc = (27573648-1,27573668) \n",
    "g11_loc = (27573547-1,27573567)\n",
    "g12_loc = (27573546-1,27573566)\n",
    "g13_loc = (27573545-1,27573565)\n",
    "repeat = (27573528-1, 27573547)\n",
    "\n",
    "def del_types(inbam, group = 'None'):\n",
    "    deletions_lst, insertions_lst = find_deletion_locs(inbam) #adding an insertion list!\n",
    "\n",
    "    #the left guide in this case was the c\n",
    "    lg_loc = (27573507-1,27573527)\n",
    "    rg_loc = (0,0)\n",
    "\n",
    "    if group.endswith('g4'):\n",
    "        rg_loc = g4_loc\n",
    "    elif group.endswith('g11'):\n",
    "        rg_loc = g11_loc\n",
    "    elif group.endswith('g12'):\n",
    "        rg_loc = g12_loc\n",
    "    elif group.endswith('g13'):\n",
    "        rg_loc = g13_loc\n",
    "\n",
    "    num_reads = len(deletions_lst)\n",
    "\n",
    "    #exisions lists\n",
    "    exision_intronic_lst = [0 for i in range(num_reads)]\n",
    "    exision_ex1_lst = [0 for i in range(num_reads)]\n",
    "    exision_ex2_lst = [0 for i in range(num_reads)]\n",
    "    exision_ex1_ex2_lst = [0 for i in range(num_reads)]\n",
    "\n",
    "    #deletion lists\n",
    "    intronic_del_lst = [0 for i in range(num_reads)] #not including gRNA loc\n",
    "    ex1_del_lst = [0 for i in range(num_reads)]\n",
    "    ex2_del_lst = [0 for i in range(num_reads)]\n",
    "    guide_del_lst = [0 for i in range(num_reads)] #deletion at either grna target site\n",
    "\n",
    "    #insertion lists\n",
    "    intronic_ins_lst = [0 for i in range(num_reads)] #not including gRNA loc\n",
    "    ex1_ins_lst = [0 for i in range(num_reads)]\n",
    "    ex2_ins_lst = [0 for i in range(num_reads)]\n",
    "    guide_ins_lst = [0 for i in range(num_reads)] #insertion at either grna target site\n",
    "\n",
    "    nothing = 0\n",
    "    if len(deletions_lst) == 0:\n",
    "        nothing = 1\n",
    "\n",
    "    index = 0\n",
    "    for read in deletions_lst:\n",
    "        found_ex1_indel = False\n",
    "        found_ex2_indel = False\n",
    "        found_ex1_ex2_indel = False\n",
    "        found_intronic_indel = False\n",
    "\n",
    "        for deletion in read:\n",
    "            #the deletion is intronic (1)\n",
    "            if between[0] < deletion[0] and deletion[1] < between[1]:\n",
    "                #spans the repeat region\n",
    "                if deletion[0] <= repeat[0] and deletion[1] >= repeat[1]:\n",
    "                    exision_intronic_lst[index] = 1\n",
    "                else:\n",
    "                    if (rg_loc[0] < deletion[0] <= rg_loc[1]) or (rg_loc[0] < deletion[1] <= rg_loc[1]) or (lg_loc[0] < deletion[1] <= lg_loc[1]) or (lg_loc[0] < deletion[0] <= lg_loc[1]):\n",
    "                        guide_del_lst[index] += 1\n",
    "                    elif (deletion[0] <= rg_loc[0] and deletion[1] >= rg_loc[0]) or (deletion[0] <= rg_loc[1] and deletion[1] >= rg_loc[1]):\n",
    "                        guide_del_lst[index] += 1\n",
    "                    elif (deletion[0] <= lg_loc[0] and deletion[1] >= lg_loc[0]) or (deletion[0] <= lg_loc[1] and deletion[1] >= lg_loc[1]):\n",
    "                        guide_del_lst[index] += 1\n",
    "                    else:\n",
    "                        intronic_del_lst[index] += 1\n",
    "\n",
    "            #deletion slices out some of the exon\n",
    "            else:\n",
    "                #spans both exons (3)\n",
    "                if deletion[0] < ex1_coord[1] and deletion[1] > ex2_coord[0]:\n",
    "                    exision_ex1_ex2_lst[index] += 1\n",
    "                #excision which overlaps with ex1, but not ex2 (4)\n",
    "                elif deletion[0] <= ex1_coord[1] and deletion[1] >= repeat[1]:\n",
    "                    exision_ex1_lst[index] += 1\n",
    "                #overlaps with ex2 (not ex1) (5)\n",
    "                elif deletion[0] <= repeat[0] and deletion[1] >= ex2_coord[0]:\n",
    "                    exision_ex2_lst[index] += 1\n",
    "                #deletion which overlaps with exon 1 (but doesn't lead to an excision) (6)\n",
    "                elif deletion[0] < ex1_coord[1]:\n",
    "                    ex1_del_lst[index] += 1\n",
    "                #deletion which overlaps with exon 2 (but doesn't lead to an excision) (7)\n",
    "                elif deletion[0] > ex2_coord[0]:\n",
    "                    ex2_del_lst[index] += 1\n",
    "        index += 1\n",
    "\n",
    "    index = 0\n",
    "\n",
    "    for read in insertions_lst:\n",
    "        print('insertion', read)\n",
    "        for insertion in read:\n",
    "            #the insertion is completley intronic\n",
    "            if between[0] < insertion[0] and insertion[1] < between[1]:\n",
    "                if (rg_loc[0] < insertion[0] <= rg_loc[1]) or (rg_loc[0] < insertion[1] <= rg_loc[1]) or (lg_loc[0] < insertion[1] <= lg_loc[1]) or (lg_loc[0] < insertion[0] <= lg_loc[1]):\n",
    "                        guide_ins_lst[index] += 1\n",
    "                else:\n",
    "                    intronic_ins_lst[index] += 1\n",
    "            else:\n",
    "                #the insertion overlaps with exon 1\n",
    "                if insertion[0] < ex1_coord[1]:\n",
    "                    ex1_ins_lst[index] += 1\n",
    "                #the insertion overlaps with exon 2 (make sure this isn't outside the exon\n",
    "                elif insertion[1] > ex2_coord[0]:\n",
    "                    ex2_ins_lst[index] += 1\n",
    "        index += 1\n",
    "\n",
    "    read_data = pd.DataFrame()\n",
    "    read_data['group'] = [group for i in range(len(deletions_lst))]\n",
    "    read_data['name'] = [read.query_name for read in inbam.fetch(\"chr9\", 27557179, 27580459) if read.reference_start < ex1_coord[0] or read.query_name in inversion_reads]\n",
    "    read_data['Intronic Excision?'] = exision_intronic_lst\n",
    "    read_data['excision which overlaps with ex1'] = exision_ex1_lst\n",
    "    read_data['excision which overlaps with ex2'] = exision_ex2_lst\n",
    "    read_data['excision which overlaps with ex1 and ex2'] = exision_ex1_ex2_lst\n",
    "    read_data['intronic deletions'] = intronic_del_lst\n",
    "    read_data['guide site deletion'] = guide_del_lst\n",
    "    read_data[\"deletions in ex1\"] = ex1_del_lst\n",
    "    read_data[\"deletions in ex2\"] = ex2_del_lst\n",
    "    read_data[\"insertions in ex1\"] = ex1_ins_lst\n",
    "    read_data[\"insertions in ex2\"] = ex2_ins_lst\n",
    "    read_data['intronic insertions'] = intronic_ins_lst\n",
    "    read_data['guide site insertion'] = guide_ins_lst\n",
    "\n",
    "    return read_data, (sum(exision_intronic_lst),\n",
    "                       sum(exision_ex1_lst),\n",
    "                       sum(exision_ex2_lst),\n",
    "                       sum(exision_ex1_ex2_lst),\n",
    "                       sum(intronic_del_lst),\n",
    "                      sum(ex1_del_lst),\n",
    "                       sum(ex2_del_lst),\n",
    "                      nothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487b0e96-dea8-4e49-b868-d1cf3426c0a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#identifies the deletion types (and insertions) in each of the sample files\n",
    "\n",
    "structure = {'g4': None,\n",
    "              'g11': None,\n",
    "             'g12': None,\n",
    "              'g13': None}\n",
    "\n",
    "# Create separate copies of the structure\n",
    "c9p4_mutant = copy.deepcopy(structure)\n",
    "c9p5_mutant = copy.deepcopy(structure)\n",
    "c9p8_mutant = copy.deepcopy(structure)\n",
    "c9p9_mutant = copy.deepcopy(structure)\n",
    "\n",
    "c9p4_wt = copy.deepcopy(structure)\n",
    "c9p5_wt = copy.deepcopy(structure)\n",
    "c9p8_wt = copy.deepcopy(structure)\n",
    "c9p9_wt = copy.deepcopy(structure)\n",
    "\n",
    "#___________c9p4______________\n",
    "c9p4_mut_rd_g4,c9p4_mutant['g4'] = del_types((pysam.AlignmentFile('c9p4_samples_merged/MUTANT_Biosample_1.fofn.pbmm2.repeats.bam%.bam.merged.bam', \"rb\")), 'c9p4_mut_g4')\n",
    "c9p4_mut_rd_g11,c9p4_mutant['g11'] = del_types((pysam.AlignmentFile('c9p4_samples_merged/MUTANT_Biosample_2.fofn.pbmm2.repeats.bam%.bam.merged.bam', \"rb\")), 'c9p4_mut_g11')\n",
    "c9p4_mut_rd_g12,c9p4_mutant['g12']  = del_types((pysam.AlignmentFile('c9p4_samples_merged/MUTANT_Biosample_3.fofn.pbmm2.repeats.bam%.bam.merged.bam', \"rb\")), 'c9p4_mut_g12')\n",
    "c9p4_mut_rd_g13,c9p4_mutant['g13']  = del_types((pysam.AlignmentFile('c9p4_samples_merged/MUTANT_Biosample_4.fofn.pbmm2.repeats.bam%.bam.merged.bam', \"rb\")), 'c9p4_mut_g13')\n",
    "\n",
    "c9p4_wt_rd_g4, c9p4_wt['g4'] = del_types((pysam.AlignmentFile('c9p4_samples_merged/WT_Biosample_1.fofn.pbmm2.repeats.bam%.bam.merged.bam', \"rb\")),'c9p4_wt_g4')\n",
    "c9p4_wt_rd_g11, c9p4_wt['g11'] = del_types((pysam.AlignmentFile('c9p4_samples_merged/WT_Biosample_2.fofn.pbmm2.repeats.bam%.bam.merged.bam', \"rb\")), 'c9p4_wt_g11')\n",
    "c9p4_wt_rd_g12, c9p4_wt['g12'] = del_types((pysam.AlignmentFile('c9p4_samples_merged/WT_Biosample_3.fofn.pbmm2.repeats.bam%.bam.merged.bam', \"rb\")), 'c9p4_wt_g12')\n",
    "c9p4_wt_rd_g13, c9p4_wt['g13'] = del_types((pysam.AlignmentFile('c9p4_samples_merged/WT_Biosample_4.fofn.pbmm2.repeats.bam%.bam.merged.bam', \"rb\")), 'c9p4_wt_g13')\n",
    "\n",
    "#___________c9p5________________\n",
    "c9p5_mut_rd_g4, c9p5_mutant['g4'] = del_types((pysam.AlignmentFile('c9p5_samples_merged/MUTANT_Biosample_5.fofn.pbmm2.repeats.bam%.bam.merged.bam', \"rb\")), 'c9p5_mut_g4')\n",
    "c9p5_mut_rd_g11, c9p5_mutant['g11']= del_types((pysam.AlignmentFile('c9p5_samples_merged/MUTANT_Biosample_6.fofn.pbmm2.repeats.bam%.bam.merged.bam', \"rb\")),'c9p5_mut_g11')\n",
    "c9p5_mut_rd_g12,c9p5_mutant['g12'] = del_types((pysam.AlignmentFile('c9p5_samples_merged/MUTANT_Biosample_7.fofn.pbmm2.repeats.bam%.bam.merged.bam', \"rb\")), 'c9p5_mut_g12')\n",
    "c9p5_mut_rd_g13,c9p5_mutant['g13'] = del_types((pysam.AlignmentFile('c9p5_samples_merged/MUTANT_Biosample_8.fofn.pbmm2.repeats.bam%.bam.merged.bam', \"rb\")), 'c9p5_mut_g13')\n",
    "\n",
    "\n",
    "c9p5_wt_rd_g4,c9p5_wt['g4'] = del_types((pysam.AlignmentFile('c9p5_samples_merged/WT_Biosample_5.fofn.pbmm2.repeats.bam%.bam.merged.bam', \"rb\")), 'c9p5_wt_g4')\n",
    "c9p5_wt_rd_g11,c9p5_wt['g11']  = del_types((pysam.AlignmentFile('c9p5_samples_merged/WT_Biosample_6.fofn.pbmm2.repeats.bam%.bam.merged.bam', \"rb\")), 'c9p5_wt_g11')\n",
    "c9p5_wt_rd_g12,c9p5_wt['g12']  = del_types((pysam.AlignmentFile('c9p5_samples_merged/WT_Biosample_7.fofn.pbmm2.repeats.bam%.bam.merged.bam', \"rb\")), 'c9p5_wt_g12')\n",
    "c9p5_wt_rd_g13,c9p5_wt['g13']  = del_types((pysam.AlignmentFile('c9p5_samples_merged/WT_Biosample_8.fofn.pbmm2.repeats.bam%.bam.merged.bam', \"rb\")), 'c9p5_wt_g13')\n",
    "\n",
    "#_________c9p8__________________\n",
    "c9p8_mut_rd_g4,c9p8_mutant['g4'] = del_types((pysam.AlignmentFile('c9p8_samples_merged/MUTANT_Biosample_9.fofn.pbmm2.repeats.bam%.bam.merged.bam', \"rb\")), 'c9p8_mut_g4')\n",
    "c9p8_mut_rd_g11,c9p8_mutant['g11'] = del_types((pysam.AlignmentFile('c9p8_samples_merged/MUTANT_Biosample_10.fofn.pbmm2.repeats.bam%.bam.merged.bam', \"rb\")),'c9p8_mut_g11')\n",
    "c9p8_mut_rd_g12,c9p8_mutant['g12'] = del_types((pysam.AlignmentFile('c9p8_samples_merged/MUTANT_Biosample_11.fofn.pbmm2.repeats.bam%.bam.merged.bam', \"rb\")), 'c9p8_mut_g12')\n",
    "c9p8_mut_rd_g13,c9p8_mutant['g13'] = del_types((pysam.AlignmentFile('c9p8_samples_merged/MUTANT_Biosample_12.fofn.pbmm2.repeats.bam%.bam.merged.bam', \"rb\")), 'c9p8_mut_g13')\n",
    "\n",
    "\n",
    "c9p8_wt_rd_g4,c9p8_wt['g4'] = del_types((pysam.AlignmentFile('c9p8_samples_merged/WT_Biosample_9.fofn.pbmm2.repeats.bam%.bam.merged.bam', \"rb\")),'c9p8_wt_g4' )\n",
    "c9p8_wt_rd_g11,c9p8_wt['g11'] = del_types((pysam.AlignmentFile('c9p8_samples_merged/WT_Biosample_10.fofn.pbmm2.repeats.bam%.bam.merged.bam', \"rb\")),'c9p8_wt_g11')\n",
    "c9p8_wt_rd_g12,c9p8_wt['g12'] = del_types((pysam.AlignmentFile('c9p8_samples_merged/WT_Biosample_11.fofn.pbmm2.repeats.bam%.bam.merged.bam', \"rb\")),'c9p8_wt_g12')\n",
    "c9p8_wt_rd_g13,c9p8_wt['g13'] = del_types((pysam.AlignmentFile('c9p8_samples_merged/WT_Biosample_12.fofn.pbmm2.repeats.bam%.bam.merged.bam', \"rb\")),'c9p8_wt_g13')\n",
    "\n",
    "\n",
    "#_______c9p9____________________\n",
    "c9p9_mut_rd_g4,c9p9_mutant['g4'] = del_types((pysam.AlignmentFile('c9p9_samples_merged/bam_1_13_MUTANT_sort.bam', \"rb\")), 'c9p9_mut_g4')\n",
    "c9p9_mut_rd_g11,c9p9_mutant['g11'] = del_types((pysam.AlignmentFile('c9p9_samples_merged/bam_2_14_MUTANT_sort.bam', \"rb\")),'c9p9_mut_g11')\n",
    "c9p9_mut_rd_g12,c9p9_mutant['g12'] = del_types((pysam.AlignmentFile('c9p9_samples_merged/bam_3_15_MUTANT_sort.bam', \"rb\")), 'c9p9_mut_g12')\n",
    "c9p9_mut_rd_g13,c9p9_mutant['g13'] = del_types((pysam.AlignmentFile('c9p9_samples_merged/bam_4_16_MUTANT_sort.bam', \"rb\")), 'c9p9_mut_g13')\n",
    "\n",
    "c9p9_wt_rd_g4,c9p9_wt['g4'] = del_types((pysam.AlignmentFile('c9p9_samples_merged/bam_1_13_WT_sort.bam', \"rb\")),'c9p9_wt_g4' )\n",
    "c9p9_wt_rd_g11,c9p9_wt['g11'] = del_types((pysam.AlignmentFile('c9p9_samples_merged/bam_2_14_WT_sort.bam', \"rb\")),'c9p9_wt_g11')\n",
    "c9p9_wt_rd_g12,c9p9_wt['g12'] = del_types((pysam.AlignmentFile('c9p9_samples_merged/bam_3_15_WT_sort.bam', \"rb\")),'c9p9_wt_g12')\n",
    "c9p9_wt_rd_g13,c9p9_wt['g13'] = del_types((pysam.AlignmentFile('c9p9_samples_merged/bam_4_16_WT_sort.bam', \"rb\")),'c9p9_wt_g13')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10403ace-f479-49e3-ab48-93773d33bb07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#creates a combined table with all the \n",
    "read_table = pd.concat([c9p4_mut_rd_g4,\n",
    "                       c9p4_mut_rd_g11,\n",
    "                       c9p4_mut_rd_g12,\n",
    "                       c9p4_mut_rd_g13,\n",
    "                       c9p4_wt_rd_g4,\n",
    "                       c9p4_wt_rd_g11,\n",
    "                       c9p4_wt_rd_g12,\n",
    "                       c9p4_wt_rd_g13,\n",
    "                       c9p5_mut_rd_g4,\n",
    "                       c9p5_mut_rd_g11,\n",
    "                       c9p5_mut_rd_g12,\n",
    "                       c9p5_mut_rd_g13,\n",
    "                       c9p5_wt_rd_g4,\n",
    "                       c9p5_wt_rd_g11,\n",
    "                       c9p5_wt_rd_g12,\n",
    "                       c9p5_wt_rd_g13,\n",
    "                       c9p8_mut_rd_g4,\n",
    "                       c9p8_mut_rd_g11,\n",
    "                       c9p8_mut_rd_g12,\n",
    "                       c9p8_mut_rd_g13,\n",
    "                       c9p8_wt_rd_g4,\n",
    "                       c9p8_wt_rd_g11,\n",
    "                       c9p8_wt_rd_g12,\n",
    "                       c9p8_wt_rd_g13,\n",
    "                       c9p9_mut_rd_g4,\n",
    "                       c9p9_mut_rd_g11,\n",
    "                       c9p9_mut_rd_g12,\n",
    "                       c9p9_mut_rd_g13,\n",
    "                       c9p9_wt_rd_g4,\n",
    "                       c9p9_wt_rd_g11,\n",
    "                       c9p9_wt_rd_g12,\n",
    "                       c9p9_wt_rd_g13])\n",
    "\n",
    "read_table['inversion'] = [0 for i in range(len(read_table))]\n",
    "read_table.loc[read_table['name'].isin(inversion_reads), 'inversion'] = int(1)\n",
    "# Save the DataFrame to a CSV file\n",
    "read_table.to_csv('detailed_read_events.csv', index=False)\n",
    "read_table.groupby('group').sum().reset_index().to_csv('detailed_read_events_summary.csv', index=False)\n",
    "read_table\n",
    "\n",
    "\n",
    "#a different way of categorizign the reads\n",
    "\n",
    "# Desired excision (HRE deletion with no disruption of exons)\n",
    "# Undesired excision type I (HRE deletion extending into either exon)\n",
    "# Undesired excision type II (HRE deletion with indel at either exon)\n",
    "# Indel type I (no HRE deletion with indel at either exon)\n",
    "# Indel type II (no HRE deletion with indel at either gRNA target site)\n",
    "# Indel type III (no HRE deletion with indel outside of exons or target sites)\n",
    "# Indel type IV (two or more indels?)\n",
    "# Inversion (with or without any indels)\n",
    "# Wild-type (unedited sequence)\n",
    "#now using grouping and such - making the \"mutually exclusive read table\"\n",
    "mut_exc_read_table = pd.DataFrame(columns = ['Group',\n",
    "                                  'Name',\n",
    "                                  'Desired excision',\n",
    "                                 'Undesired excision type I',\n",
    "                                 'Undesired excision type II',\n",
    "                                 'Indel type I',\n",
    "                                 'Indel type II',\n",
    "                                 'Indel type III',\n",
    "                                 'Indel Type IV',\n",
    "                                 'Inversion',\n",
    "                                 'Wild-type'])\n",
    "\n",
    "\n",
    "#iterating through all the types of edits, and all the reads and then adding a row to the major table = depending on their idenitity\n",
    "for index, row in read_table.iterrows():\n",
    "    #inversion\n",
    "    new_rows = []\n",
    "    if row['inversion']:\n",
    "        new_row = {\n",
    "            'Group': row['group'],\n",
    "            'Name': row['name'],\n",
    "            'Desired excision': 0,\n",
    "            'Undesired excision type I': 0,\n",
    "            'Undesired excision type II': 0,\n",
    "            'Indel type I': 0,\n",
    "            'Indel type II': 0,\n",
    "            'Indel type III': 0,\n",
    "            'Indel Type IV': 0,\n",
    "            'Inversion': 1,\n",
    "            'Wild-type': 0\n",
    "        }\n",
    "\n",
    "    #undesired excision type I (overlaps with an exon)\n",
    "    elif row['excision which overlaps with ex1'] or  row['excision which overlaps with ex2'] or row['excision which overlaps with ex1 and ex2']:\n",
    "        new_row = {\n",
    "            'Group': row['group'],\n",
    "            'Name': row['name'],\n",
    "            'Desired excision': 0,\n",
    "            'Undesired excision type I': 1,\n",
    "            'Undesired excision type II': 0,\n",
    "            'Indel type I': 0,\n",
    "            'Indel type II': 0,\n",
    "            'Indel type III': 0,\n",
    "            'Indel Type IV': 0,\n",
    "            'Inversion': 0,\n",
    "            'Wild-type': 0\n",
    "        }\n",
    "\n",
    "    #undesired excision type II (also an indel in the exon)\n",
    "    elif row['Intronic Excision?'] and (row['deletions in ex1'] or\n",
    "                                        row['deletions in ex2'] or\n",
    "                                        row['insertions in ex1'] or\n",
    "                                        row['insertions in ex2']):\n",
    "        new_row = {\n",
    "            'Group': row['group'],\n",
    "            'Name': row['name'],\n",
    "            'Desired excision': 0,\n",
    "            'Undesired excision type I': 0,\n",
    "            'Undesired excision type II': 1,\n",
    "            'Indel type I': 0,\n",
    "            'Indel type II': 0,\n",
    "            'Indel type III': 0,\n",
    "            'Indel Type IV': 0,\n",
    "            'Inversion': 0,\n",
    "            'Wild-type': 0\n",
    "        }\n",
    "    #desired excision\n",
    "    elif row['Intronic Excision?']:\n",
    "        new_row = {\n",
    "            'Group': row['group'],\n",
    "            'Name': row['name'],\n",
    "            'Desired excision': 1,\n",
    "            'Undesired excision type I': 0,\n",
    "            'Undesired excision type II': 0,\n",
    "            'Indel type I': 0,\n",
    "            'Indel type II': 0,\n",
    "            'Indel type III': 0,\n",
    "            'Indel Type IV': 0,\n",
    "            'Inversion': 0,\n",
    "            'Wild-type': 0\n",
    "        }\n",
    "\n",
    "    #no excision, but an indel in one of the exons (only)\n",
    "    elif (row['deletions in ex2'] or row['deletions in ex1'] or row['insertions in ex1'] or row['insertions in ex2']) and not row['intronic insertions'] and not row['intronic deletions'] and not row['guide site insertion'] and not row['guide site deletion']:\n",
    "        new_row = {\n",
    "            'Group': row['group'],\n",
    "            'Name': row['name'],\n",
    "            'Desired excision': 0,\n",
    "            'Undesired excision type I': 0,\n",
    "            'Undesired excision type II': 0,\n",
    "            'Indel type I': 1,\n",
    "            'Indel type II': 0,\n",
    "            'Indel type III': 0,\n",
    "            'Indel Type IV': 0,\n",
    "            'Inversion': 0,\n",
    "            'Wild-type': 0\n",
    "        }\n",
    "\n",
    "    #no excision, but intronic deletion/insetion but NOT at the guide sites and not in the exonds\n",
    "    elif row['intronic deletions'] or row['intronic insertions'] and not row['guide site insertion'] and not row['guide site deletion'] and not (row['deletions in ex2'] or row['deletions in ex1'] or row['insertions in ex1'] or row['insertions in ex2']):\n",
    "        new_row = {\n",
    "            'Group': row['group'],\n",
    "            'Name': row['name'],\n",
    "            'Desired excision': 0,\n",
    "            'Undesired excision type I': 0,\n",
    "            'Undesired excision type II': 0,\n",
    "            'Indel type I': 0,\n",
    "            'Indel type II': 0,\n",
    "            'Indel type III': 1,\n",
    "            'Indel Type IV': 0,\n",
    "            'Inversion': 0,\n",
    "            'Wild-type': 0\n",
    "        }\n",
    "\n",
    "    #alternativey, JUST a guide site insetion\n",
    "    elif row['guide site insertion'] or row['guide site deletion'] and not (row['intronic deletions'] or row['intronic insertions'] or row['deletions in ex2'] or row['deletions in ex1'] or row['insertions in ex1'] or row['insertions in ex2']):\n",
    "        new_row = {\n",
    "            'Group': row['group'],\n",
    "            'Name': row['name'],\n",
    "            'Desired excision': 0,\n",
    "            'Undesired excision type I': 0,\n",
    "            'Undesired excision type II': 0,\n",
    "            'Indel type I': 0,\n",
    "            'Indel type II': 1,\n",
    "            'Indel type III': 0,\n",
    "            'Indel Type IV': 0,\n",
    "            'Inversion': 0,\n",
    "            'Wild-type': 0\n",
    "        }\n",
    "    #check if the remaining columns are 0 or not\n",
    "    elif row.iloc[2:].sum() == 0:\n",
    "        new_row = {\n",
    "            'Group': row['group'],\n",
    "            'Name': row['name'],\n",
    "            'Desired excision': 0,\n",
    "            'Undesired excision type I': 0,\n",
    "            'Undesired excision type II': 0,\n",
    "            'Indel type I': 0,\n",
    "            'Indel type II': 0,\n",
    "            'Indel type III': 0,\n",
    "            'Indel Type IV': 0,\n",
    "            'Inversion': 0,\n",
    "            'Wild-type': 1\n",
    "        }\n",
    "    else:\n",
    "        new_row = {\n",
    "            'Group': row['group'],\n",
    "            'Name': row['name'],\n",
    "            'Desired excision': 0,\n",
    "            'Undesired excision type I': 0,\n",
    "            'Undesired excision type II': 0,\n",
    "            'Indel type I': 0,\n",
    "            'Indel type II': 0,\n",
    "            'Indel type III': 0,\n",
    "            'Indel Type IV': 1,\n",
    "            'Inversion': 0,\n",
    "            'Wild-type': 0\n",
    "        }\n",
    "\n",
    "    #adding the new row to the table!\n",
    "    # Convert the new_row dictionary to a DataFrame and append it\n",
    "    new_row_df = pd.DataFrame([new_row])\n",
    "    mut_exc_read_table = pd.concat([mut_exc_read_table, new_row_df], ignore_index=True)\n",
    "\n",
    "\n",
    "read_table.to_csv(\"massive_read_table.csv\", index = False)\n",
    "mut_exc_read_table.to_csv(\"bankole_categorical_breakdown_reads.csv\", index=False)\n",
    "\n",
    "groupedby_group = mut_exc_read_table.groupby('Group').sum().reset_index()\n",
    "groupedby_group = groupedby_group.drop(\"Name\", axis = 1)\n",
    "groupedby_group.to_csv(\"bankole_categorical_breakdown_summary.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd6dc7d-9985-4cc3-9d0d-b92594c71879",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "global_value = 0\n",
    "# Function to generate filenames\n",
    "def generate_filename(directory='images', extension='png'):\n",
    "    global global_value  # Declare that we are using the global variable\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    global_value += 1  # Increment the global value\n",
    "    return os.path.join(directory, f'file_{global_value}.{extension}')\n",
    "\n",
    "\n",
    "#making plots for the more coarse grained situation (desired excision, undesired excision,  inversion, wt, indel - indels\n",
    "#might also be sequencing errors\n",
    "corse_categories = ['Desired Excison', 'Undesired Excision', 'Indel Extending into Exonic Region']\n",
    "read_table_corse = pd.DataFrame()\n",
    "read_table_corse['Group'] = mut_exc_read_table['Group']\n",
    "read_table_corse['Name'] = mut_exc_read_table['Name']\n",
    "read_table_corse['Desired excision'] = mut_exc_read_table['Desired excision']\n",
    "#excision that extends into exon1 or exon 2\n",
    "read_table_corse['Undesired Excision'] = mut_exc_read_table['Undesired excision type I'] + mut_exc_read_table['Undesired excision type II']\n",
    "#inversions (which I manually identified)\n",
    "read_table_corse['Inversion'] = mut_exc_read_table['Inversion']\n",
    "#indels which are in the guide region\n",
    "read_table_corse['Indel'] =  mut_exc_read_table['Indel type II']\n",
    "#wild type! (ALL else)\n",
    "read_table_corse['Wildtype'] = mut_exc_read_table['Wild-type'] + mut_exc_read_table['Indel type III']\n",
    "\n",
    "print(sum(read_table_corse['Desired excision']))\n",
    "print(sum(read_table_corse['Undesired Excision']))\n",
    "print(sum(read_table_corse['Inversion']))\n",
    "print(sum(read_table_corse['Indel']))\n",
    "print(sum(read_table_corse['Wildtype']))\n",
    "\n",
    "display(read_table_corse)\n",
    "read_table_corse.to_csv(\"coarse_breakdown_reads.csv\", index=False)\n",
    "read_table_corse_by_group = read_table_corse.groupby('Group').sum().reset_index()\n",
    "read_table_corse_by_group.to_csv(\"coarse_breakdown_summary.csv\", index=False)\n",
    "\n",
    "grouped_sum = read_table_corse.groupby('Group').sum()\n",
    "grouped_sum = grouped_sum.reset_index()\n",
    "grouped_sum['Cell Line'] = [name[:4] for name in grouped_sum['Group']]\n",
    "grouped_sum['Guide'] = [name[-3:] for name in grouped_sum['Group']]\n",
    "grouped_sum['Allele'] = [name[5:8] for name in grouped_sum['Group']]\n",
    "\n",
    "mut_exc_read_table['Cell Line'] = [name[:4] for name in mut_exc_read_table['Group']]\n",
    "mut_exc_read_table['Guide'] = [name[-3:] for name in mut_exc_read_table['Group']]\n",
    "mut_exc_read_table['Allele'] = [name[5:8] for name in mut_exc_read_table['Group']]\n",
    "\n",
    "#seperating by cell line\n",
    "c9p4_corse = grouped_sum[grouped_sum['Cell Line'] == 'c9p4']\n",
    "c9p5_course = grouped_sum[grouped_sum['Cell Line'] == 'c9p5']\n",
    "c9p8_course = grouped_sum[grouped_sum['Cell Line'] == 'c9p8']\n",
    "c9p9_course = grouped_sum[grouped_sum['Cell Line'] == 'c9p9']\n",
    "\n",
    "#seperating by allele type\n",
    "c9p4_corse_wt = c9p4_corse[c9p4_corse['Allele'] == 'wt_']\n",
    "c9p5_corse_wt = c9p5_course[c9p5_course['Allele'] == 'wt_']\n",
    "c9p8_corse_wt = c9p8_course[c9p8_course['Allele'] == 'wt_']\n",
    "c9p9_corse_wt = c9p9_course[c9p9_course['Allele'] == 'wt_']\n",
    "\n",
    "c9p4_corse_mut = c9p4_corse[c9p4_corse['Allele'] == 'mut']\n",
    "c9p5_corse_mut = c9p5_course[c9p5_course['Allele'] == 'mut']\n",
    "c9p8_corse_mut = c9p8_course[c9p8_course['Allele'] == 'mut']\n",
    "c9p9_corse_mut = c9p9_course[c9p9_course['Allele'] == 'mut']\n",
    "\n",
    "\n",
    "def corse_stacked_bar(tbl, column_names, title = None, ylabel = 'Number of Reads', bars = 'Guide', y_lim = (0,1.1)):\n",
    "    y_labels = tbl[bars]\n",
    "    colors = [\n",
    "    '#d0f0c0',  # Pale Green\n",
    "    '#a3d9a5',  # Light Moss Green\n",
    "    '#7cc57f',  # Light Olive Green\n",
    "    '#66cdaa',  # Medium Aquamarine\n",
    "    '#3cb371',  # Medium Sea Green\n",
    "    '#2e8b57',  # Sea Green\n",
    "    '#228b22',  # Forest Green\n",
    "    '#006400',  # Dark Green\n",
    "    '#004d00'   # Very Dark Green\n",
    "]\n",
    "\n",
    "    bottoms = [0] * len(y_labels)\n",
    "    for i in range(len(column_names)):\n",
    "        column_name = column_names[i]\n",
    "        plt.bar(y_labels, tbl[column_name], bottom=bottoms, color=colors[i], label=f'Segment {i+1}', edgecolor='black', linewidth=1.6)\n",
    "        # Update bottoms to include the height of the newly plotted segment\n",
    "        bottoms = [b + s for b, s in zip(bottoms, tbl[column_name])]\n",
    "        plt.ylim(y_lim)\n",
    "\n",
    "\n",
    "    plt.legend(column_names,\n",
    "               loc = 'upper right',\n",
    "              frameon=False,\n",
    "              bbox_to_anchor = (1.8,1))\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel('Guide')\n",
    "    plt.ylabel(ylabel)\n",
    "       # Save and show the plot\n",
    "    filename = generate_filename()\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "for cell_line in mut_exc_read_table['Cell Line'].unique():\n",
    "    all_numeric_cols = ['Desired excision',\n",
    "                                 'Undesired excision type I',\n",
    "                                 'Undesired excision type II',\n",
    "                                 'Indel type I',\n",
    "                                 'Indel type II',\n",
    "                                 'Indel type III',\n",
    "                                 'Indel Type IV',\n",
    "                                 'Inversion',\n",
    "                                 'Wild-type']\n",
    "\n",
    "    #extract the table for the cell line of interest\n",
    "    tbl = mut_exc_read_table[mut_exc_read_table['Cell Line'] == cell_line]\n",
    "    tbl = tbl.groupby(['Guide', 'Allele']).sum()\n",
    "    tbl.reset_index()\n",
    "\n",
    "    tbl['Cell Line'] = [name[:4] for name in tbl['Group']]\n",
    "    tbl['Guide'] = [name[-3:] for name in tbl['Group']]\n",
    "    tbl['Allele'] = [name[5:8] for name in tbl['Group']]\n",
    "\n",
    "    tbl_wt = tbl[tbl['Allele'] == 'wt_']\n",
    "    tbl_mut = tbl[tbl['Allele'] == 'mut']\n",
    "\n",
    "    tbl_row_sums_wt = tbl_wt[all_numeric_cols].sum(axis=1)\n",
    "    tbl_row_sums_mut = tbl_mut[all_numeric_cols].sum(axis=1)\n",
    "\n",
    "    tbl_wt_norm = tbl_wt[all_numeric_cols].div(tbl_row_sums_wt, axis = 0) #normalizing the wt tbl\n",
    "    tbl_mut_norm = tbl_mut[all_numeric_cols].div(tbl_row_sums_mut, axis = 0) #normalizing the mut tbl\n",
    "\n",
    "    tbl_wt_norm['Group'] = tbl_wt['Group']\n",
    "    tbl_mut_norm['Group'] = tbl_mut['Group']\n",
    "\n",
    "    #adding the non-numeric columns back to the tables\n",
    "    tbl_wt_norm['Cell Line'] = [name[:4] for name in tbl_wt_norm['Group']]\n",
    "    tbl_wt_norm['Guide'] = [name[-3:] for name in tbl_wt_norm['Group']]\n",
    "    tbl_wt_norm['Allele'] = [name[5:8] for name in tbl_wt_norm['Group']]\n",
    "\n",
    "    #adding the non-numeric columns back to the tables\n",
    "    tbl_mut_norm['Cell Line'] = [name[:4] for name in tbl_mut_norm['Group']]\n",
    "    tbl_mut_norm['Guide'] = [name[-3:] for name in tbl_mut_norm['Group']]\n",
    "    tbl_mut_norm['Allele'] = [name[5:8] for name in tbl_mut_norm['Group']]\n",
    "\n",
    "    corse_stacked_bar(tbl_wt, all_numeric_cols, title = str(cell_line) + 'wt_', y_lim = (0,25))\n",
    "    corse_stacked_bar(tbl_mut, all_numeric_cols, title = str(cell_line) + 'mut', y_lim = (0,25))\n",
    "    corse_stacked_bar(tbl_wt_norm, all_numeric_cols, title = str(cell_line) + 'wt_')\n",
    "    corse_stacked_bar(tbl_mut_norm, all_numeric_cols, title = str(cell_line) + 'mut')\n",
    "\n",
    "numeric_cols = ['Desired excision', 'Undesired Excision', 'Inversion', 'Indel', 'Wildtype']\n",
    "corse_stacked_bar(c9p4_corse_wt, numeric_cols, title = 'c9p4 WT', y_lim = (0,25))\n",
    "corse_stacked_bar(c9p5_corse_wt, numeric_cols, title = 'c9p5 WT',  y_lim = (0,25))\n",
    "corse_stacked_bar(c9p8_corse_wt, numeric_cols, title = 'c9p8 WT',  y_lim = (0,25))\n",
    "\n",
    "corse_stacked_bar(c9p9_corse_wt, numeric_cols, title = 'c9p9 WT',  y_lim = (0,25))\n",
    "corse_stacked_bar(c9p4_corse_mut, numeric_cols, title = 'c9p4 MUT',  y_lim = (0,25))\n",
    "corse_stacked_bar(c9p5_corse_mut, numeric_cols, title = 'c9p5 MUT',  y_lim = (0,25))\n",
    "corse_stacked_bar(c9p8_corse_mut, numeric_cols, title = 'c9p8 MUT',  y_lim = (0,25))\n",
    "corse_stacked_bar(c9p9_corse_mut, numeric_cols, title = 'c9p9 MUT',  y_lim = (0,25))\n",
    "\n",
    "numeric_cols = ['Desired excision', 'Undesired Excision', 'Inversion', 'Indel', 'Wildtype']\n",
    "\n",
    "c9p4_corse_wt_norm = c9p4_corse_wt[numeric_cols].div(c9p4_corse_wt[numeric_cols].sum(axis=1), axis = 0)\n",
    "c9p4_corse_wt_norm[\"Group\"] = c9p4_corse_wt[\"Group\"]\n",
    "c9p4_corse_wt_norm[\"Guide\"] = c9p4_corse_wt[\"Guide\"]\n",
    "corse_stacked_bar(c9p4_corse_wt_norm, numeric_cols, title = 'c9p4 WT')\n",
    "\n",
    "c9p5_corse_wt_norm = c9p5_corse_wt[numeric_cols].div(c9p5_corse_wt[numeric_cols].sum(axis=1), axis = 0)\n",
    "c9p5_corse_wt_norm[\"Group\"] = c9p5_corse_wt[\"Group\"]\n",
    "c9p5_corse_wt_norm[\"Guide\"] = c9p5_corse_wt[\"Guide\"]\n",
    "corse_stacked_bar(c9p5_corse_wt_norm, numeric_cols, title = 'c9p5 WT')\n",
    "\n",
    "c9p8_corse_wt_norm = c9p8_corse_wt[numeric_cols].div(c9p8_corse_wt[numeric_cols].sum(axis=1), axis = 0)\n",
    "c9p8_corse_wt_norm[\"Group\"] = c9p8_corse_wt[\"Group\"]\n",
    "c9p8_corse_wt_norm[\"Guide\"] = c9p8_corse_wt[\"Guide\"]\n",
    "corse_stacked_bar(c9p8_corse_wt_norm, numeric_cols, title = 'c9p8 WT')\n",
    "\n",
    "c9p9_corse_wt_norm = c9p9_corse_wt[numeric_cols].div(c9p9_corse_wt[numeric_cols].sum(axis=1), axis = 0)\n",
    "c9p9_corse_wt_norm[\"Group\"] = c9p9_corse_wt[\"Group\"]\n",
    "c9p9_corse_wt_norm[\"Guide\"] = c9p9_corse_wt[\"Guide\"]\n",
    "corse_stacked_bar(c9p9_corse_wt_norm, numeric_cols, title = 'c9p9 WT')\n",
    "\n",
    "c9p4_corse_mut_norm = c9p4_corse_mut[numeric_cols].div(c9p4_corse_mut[numeric_cols].sum(axis=1), axis = 0)\n",
    "c9p4_corse_mut_norm[\"Group\"] = c9p4_corse_mut[\"Group\"]\n",
    "c9p4_corse_mut_norm[\"Guide\"] = c9p4_corse_mut[\"Guide\"]\n",
    "corse_stacked_bar(c9p4_corse_mut_norm, numeric_cols, title = 'c9p4 MUT')\n",
    "\n",
    "c9p5_corse_mut_norm = c9p5_corse_wt[numeric_cols].div(c9p5_corse_wt[numeric_cols].sum(axis=1), axis = 0)\n",
    "c9p5_corse_mut_norm[\"Group\"] = c9p5_corse_mut[\"Group\"]\n",
    "c9p5_corse_mut_norm[\"Guide\"] = c9p5_corse_mut[\"Guide\"]\n",
    "corse_stacked_bar(c9p5_corse_mut_norm, numeric_cols, title = 'c9p5 MUT')\n",
    "\n",
    "c9p8_corse_mut_norm = c9p8_corse_wt[numeric_cols].div(c9p8_corse_wt[numeric_cols].sum(axis=1), axis = 0)\n",
    "c9p8_corse_mut_norm[\"Group\"] = c9p8_corse_mut[\"Group\"]\n",
    "c9p8_corse_mut_norm[\"Guide\"] = c9p8_corse_mut[\"Guide\"]\n",
    "corse_stacked_bar(c9p8_corse_mut_norm, numeric_cols, title = 'c9p8 MUT')\n",
    "\n",
    "c9p9_corse_mut_norm = c9p9_corse_wt[numeric_cols].div(c9p9_corse_wt[numeric_cols].sum(axis=1), axis = 0)\n",
    "c9p9_corse_mut_norm[\"Group\"] = c9p9_corse_mut[\"Group\"]\n",
    "c9p9_corse_mut_norm[\"Guide\"] = c9p9_corse_mut[\"Guide\"]\n",
    "corse_stacked_bar(c9p9_corse_mut_norm, numeric_cols, title = 'c9p9 MUT')\n",
    "\n",
    "#grouping them all together even more!\n",
    "combined_stacked_bar_all = grouped_sum.groupby('Guide').sum()\n",
    "combined_stacked_bar_all = combined_stacked_bar_all.reset_index()\n",
    "display()\n",
    "\n",
    "corse_stacked_bar(combined_stacked_bar_all, ['Desired excision', 'Undesired Excision', 'Inversion', 'Indel', 'Wildtype'], 'All Conditions', y_lim = (0,100))\n",
    "combined_stacked_bar_all_norm = pd.DataFrame()\n",
    "\n",
    "row_sums = combined_stacked_bar_all[['Desired excision', 'Undesired Excision', 'Inversion', 'Indel', 'Wildtype']].sum(axis=1)\n",
    "combined_stacked_bar_all_norm[['Desired excision', 'Undesired Excision', 'Inversion', 'Indel', 'Wildtype']] = combined_stacked_bar_all[['Desired excision', 'Undesired Excision', 'Inversion', 'Indel', 'Wildtype']].div(row_sums, axis=0)\n",
    "combined_stacked_bar_all_norm['Guide'] = combined_stacked_bar_all['Guide']\n",
    "combined_stacked_bar_all_norm['Allele'] = combined_stacked_bar_all['Allele']\n",
    "corse_stacked_bar(combined_stacked_bar_all_norm, numeric_cols, ylabel = '% occurence', title = 'Editing outcome Across Alleles and Patient Line', y_lim = (0,1.1))\n",
    "\n",
    "for cell_line in grouped_sum['Cell Line'].unique():\n",
    "    mut_wt_tbl = grouped_sum.groupby(['Cell Line', 'Allele']).sum().reset_index()\n",
    "    corse_stacked_bar(mut_wt_tbl[mut_wt_tbl['Cell Line'] == cell_line],\n",
    "                                 numeric_cols, bars = 'Allele', title = cell_line, y_lim = (0,80))\n",
    "\n",
    "\n",
    "    row_sums = mut_wt_tbl[mut_wt_tbl['Cell Line'] == cell_line][['Desired excision', 'Undesired Excision', 'Inversion', 'Indel', 'Wildtype']].sum(axis=1)\n",
    "    mut_wt_tbl_norm = mut_wt_tbl[mut_wt_tbl['Cell Line'] == cell_line][numeric_cols].div(row_sums, axis=0)\n",
    "    mut_wt_tbl_norm['Allele'] = mut_wt_tbl[mut_wt_tbl['Cell Line'] == cell_line]['Allele']\n",
    "\n",
    "    corse_stacked_bar(mut_wt_tbl_norm,\n",
    "                                 numeric_cols, bars = 'Allele', title = cell_line, y_lim = (0,1.1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b841d8d9-225c-434e-9043-756da676a232",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_stacked_bar_all\n",
    "combined_stacked_bar_all_norm_mut = pd.DataFrame()\n",
    "grouped_sum_mut = grouped_sum[grouped_sum['Allele'] == 'mut']\n",
    "grouped_sum_wt = grouped_sum[grouped_sum['Allele'] == 'wt_']\n",
    "combined_stacked_bar_all_mut = grouped_sum_mut.groupby('Guide').sum()\n",
    "combined_stacked_bar_all_mut = combined_stacked_bar_all_mut.reset_index()\n",
    "combined_stacked_bar_all_wt = grouped_sum_wt.groupby('Guide').sum()\n",
    "combined_stacked_bar_all_wt = combined_stacked_bar_all_wt.reset_index()\n",
    "combined_stacked_bar_all_norm_mut\n",
    "\n",
    "row_sums_mut = combined_stacked_bar_all_mut[['Desired excision', 'Undesired Excision', 'Inversion', 'Indel', 'Wildtype']].sum(axis=1)\n",
    "row_sums_wt = combined_stacked_bar_all_wt[['Desired excision', 'Undesired Excision', 'Inversion', 'Indel', 'Wildtype']].sum(axis=1)\n",
    "\n",
    "combined_stacked_bar_all_mut_norm = combined_stacked_bar_all_mut[['Desired excision', 'Undesired Excision', 'Inversion', 'Indel', 'Wildtype']].div(row_sums_mut, axis=0)\n",
    "combined_stacked_bar_all_wt_norm = combined_stacked_bar_all_wt[['Desired excision', 'Undesired Excision', 'Inversion', 'Indel', 'Wildtype']].div(row_sums_wt, axis=0)\n",
    "combined_stacked_bar_all_mut_norm['Guide'] = combined_stacked_bar_all['Guide']\n",
    "combined_stacked_bar_all_wt_norm['Guide'] = combined_stacked_bar_all['Guide']\n",
    "\n",
    "combined_stacked_bar_all_mut_norm\n",
    "corse_stacked_bar(combined_stacked_bar_all_mut_norm, numeric_cols, ylabel = '% occurence', title = 'Editing outcome across Patient Lines (mut)', y_lim = (0,1.1))\n",
    "corse_stacked_bar(combined_stacked_bar_all_wt_norm, numeric_cols, ylabel = '% occurence', title = 'Editing outcome Across Alleles and Patient Line (wt)', y_lim = (0,1.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41039eb9-e426-427b-9f2f-8e817602f4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def make_bar_chart_combined(tuple_lst_1, tuple_lst_2, tuple_lst_3, title):\n",
    "    # Convert lists of tuples into numpy arrays\n",
    "    array_1 = np.array(tuple_lst_1)\n",
    "    array_2 = np.array(tuple_lst_2)\n",
    "    array_3 = np.array(tuple_lst_3)\n",
    "\n",
    "    # Stack arrays to form a 3D array where the first dimension is the list index\n",
    "    stacked_arrays = np.stack([array_1, array_2, array_3], axis=0)\n",
    "\n",
    "    # Calculate the mean and standard deviation across the lists\n",
    "    avg_tuples = np.mean(stacked_arrays)\n",
    "    sd_tuples = np.std(stacked_arrays)\n",
    "\n",
    "    # Print averages and standard deviations\n",
    "    print(\"Average Tuples:\\n\", avg_tuples)\n",
    "    print(\"Standard Deviation Tuples:\\n\", sd_tuples)\n",
    "\n",
    "def make_bar_chart(tuple_lst, title):\n",
    "    #might just do the inversions manually....,\n",
    "    groups = ['g4', 'g11', 'g12', 'g13']\n",
    "    categories = ['Intronic Excision',\n",
    "                  \"Excision with overlap in Exon1\",\n",
    "                  \"Excision with overlap in Exon2\",\n",
    "                 \"Excision with overlap in Exon1 and Exon2\",\n",
    "                 \"Intronic Deletion\",\n",
    "                 \"Deletion in Exon1\",\n",
    "                 \"Deletion in Exon2\",\n",
    "                 \"No Deletions\"]\n",
    "\n",
    "\n",
    "    colors = [\n",
    "    '#d0f0c0',  # Pale Green\n",
    "    '#90ee90',  # Light Green\n",
    "    '#3cb371',  # Medium Sea Green\n",
    "    '#00fa9a',  # Medium Spring Green\n",
    "    '#2e8b57',  # Sea Green\n",
    "    '#228b22',  # Forest Green\n",
    "    '#556b2f',  # Dark Olive Green\n",
    "    '#006400'   # Dark Green\n",
    "    ]\n",
    "\n",
    "    bottom = [0] * len(categories)  # Initialize bottom positions for stacking\n",
    "    # Plot each segment of the bar\n",
    "\n",
    "    stacked = [[tuple_lst['g4'][0],  tuple_lst['g11'][0], tuple_lst['g12'][0], tuple_lst['g13'][0]],\n",
    "               [tuple_lst['g4'][1],  tuple_lst['g11'][1], tuple_lst['g12'][1], tuple_lst['g13'][1]],\n",
    "               [tuple_lst['g4'][2],  tuple_lst['g11'][2], tuple_lst['g12'][2], tuple_lst['g13'][2]],\n",
    "               [tuple_lst['g4'][3],  tuple_lst['g11'][3], tuple_lst['g12'][3], tuple_lst['g13'][3]],\n",
    "               [tuple_lst['g4'][4],  tuple_lst['g11'][4], tuple_lst['g12'][4], tuple_lst['g13'][4]],\n",
    "               [tuple_lst['g4'][5],  tuple_lst['g11'][5], tuple_lst['g12'][5], tuple_lst['g13'][5]],\n",
    "               [tuple_lst['g4'][6],  tuple_lst['g11'][6], tuple_lst['g12'][6], tuple_lst['g13'][6]],\n",
    "               [tuple_lst['g4'][7],  tuple_lst['g11'][7], tuple_lst['g12'][7], tuple_lst['g13'][7]]]\n",
    "\n",
    "    #THE C9P4 MUTANT GRAPH\n",
    "    # Initialize bottom heights\n",
    "    bottoms = [0] * len(groups)\n",
    "\n",
    "    # Plotting the stacked bar graph\n",
    "    for i in range(len(stacked)):\n",
    "        plt.bar(groups, stacked[i], bottom=bottoms, color=colors[i], label=f'Segment {i+1}', edgecolor = 'black', linewidth = 1.4)\n",
    "        # Update bottoms to include the height of the newly plotted segment\n",
    "        bottoms = [b + s for b, s in zip(bottoms, stacked[i])]\n",
    "\n",
    "    # Adding labels and title\n",
    "    plt.xlabel('Guide')\n",
    "    plt.ylabel('Number of Reads')\n",
    "    plt.title(title)\n",
    "\n",
    "    # Adding a legend\n",
    "    plt.legend(categories,\n",
    "              loc = 'upper right',\n",
    "              frameon=False,\n",
    "              bbox_to_anchor = (1.8,1))\n",
    "\n",
    "    # Display the graph\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# make_bar_chart(c9p4_mutant, 'c9p4_mutant')\n",
    "# make_bar_chart(c9p4_wt, 'c9p4_wildtype')\n",
    "\n",
    "# make_bar_chart(c9p8_mutant, 'c9p8_mutant')\n",
    "# make_bar_chart(c9p8_wt, 'c9p8_wildtype')\n",
    "\n",
    "# make_bar_chart(c9p5_mutant, 'c9p5_mutant')\n",
    "# make_bar_chart(c9p5_wt, 'c9p5_wildtype')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1c6b89-8c3c-4ab7-abd1-27bdf9ff1995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def make_bar_chart_combined(tuple_lst_1, tuple_lst_2, tuple_lst_3, title):\n",
    "    # Convert lists of tuples into numpy arrays\n",
    "    array_1 = np.array(tuple_lst_1)\n",
    "    array_2 = np.array(tuple_lst_2)\n",
    "    array_3 = np.array(tuple_lst_3)\n",
    "\n",
    "    # Stack arrays to form a 3D array where the first dimension is the list index\n",
    "    stacked_arrays = np.stack([array_1, array_2, array_3], axis=0)\n",
    "\n",
    "    # Calculate the mean and standard deviation across the lists\n",
    "    avg_tuples = np.mean(stacked_arrays)\n",
    "    sd_tuples = np.std(stacked_arrays)\n",
    "\n",
    "    # Print averages and standard deviations\n",
    "    print(\"Average Tuples:\\n\", avg_tuples)\n",
    "    print(\"Standard Deviation Tuples:\\n\", sd_tuples)\n",
    "    \n",
    "#normalizing by the total number of reads (which isn't a lot).....\n",
    "def make_bar_chart(tuple_lst, title):\n",
    "    #might just do the inversions manually....,\n",
    "    groups = ['g4', 'g11', 'g12', 'g13']\n",
    "    categories = ['Intronic Excision',\n",
    "                  \"Excision with overlap in Exon1\",\n",
    "                  \"Excision with overlap in Exon2\",\n",
    "                 \"Excision with overlap in Exon1 and Exon2\",\n",
    "                 \"Intronic Deletion\",\n",
    "                 \"Deletion in Exon1\",\n",
    "                 \"Deletion in Exon2\",\n",
    "                 \"No Deletions\"]\n",
    "\n",
    "\n",
    "    colors = [\n",
    "    '#d0f0c0',  # Pale Green\n",
    "    '#90ee90',  # Light Green\n",
    "    '#3cb371',  # Medium Sea Green\n",
    "    '#00fa9a',  # Medium Spring Green\n",
    "    '#2e8b57',  # Sea Green\n",
    "    '#228b22',  # Forest Green\n",
    "    '#556b2f',  # Dark Olive Green\n",
    "    '#006400'   # Dark Green\n",
    "    ]\n",
    "\n",
    "    bottom = [0] * len(categories)  # Initialize bottom positions for stacking\n",
    "    # Plot each segment of the bar\n",
    "\n",
    "    print(tuple_lst['g4'][0])\n",
    "\n",
    "    print(tuple_lst['g4'])\n",
    "    total_g4 = 1\n",
    "    total_g11 = 1\n",
    "    total_g12 = 1\n",
    "    total_g13 = 1\n",
    "\n",
    "    stacked = [[tuple_lst['g4'][0]/total_g4,  tuple_lst['g11'][0]/total_g11, tuple_lst['g12'][0]/total_g12, tuple_lst['g13'][0]/total_g13],\n",
    "               [tuple_lst['g4'][1]/total_g4,  tuple_lst['g11'][1]/total_g11, tuple_lst['g12'][1]/total_g12, tuple_lst['g13'][1]/total_g13],\n",
    "               [tuple_lst['g4'][2]/total_g4,  tuple_lst['g11'][2]/total_g11, tuple_lst['g12'][2]/total_g12, tuple_lst['g13'][2]/total_g13],\n",
    "               [tuple_lst['g4'][3]/total_g4,  tuple_lst['g11'][3]/total_g11, tuple_lst['g12'][3]/total_g12, tuple_lst['g13'][3]/total_g13],\n",
    "               [tuple_lst['g4'][4]/total_g4,  tuple_lst['g11'][4]/total_g11, tuple_lst['g12'][4]/total_g12, tuple_lst['g13'][4]/total_g13],\n",
    "               [tuple_lst['g4'][5]/total_g4,  tuple_lst['g11'][5]/total_g11, tuple_lst['g12'][5]/total_g12, tuple_lst['g13'][5]/total_g13],\n",
    "               [tuple_lst['g4'][6]/total_g4,  tuple_lst['g11'][6]/total_g11, tuple_lst['g12'][6]/total_g12, tuple_lst['g13'][6]/total_g13],\n",
    "               [tuple_lst['g4'][7]/total_g4,  tuple_lst['g11'][7]/total_g11, tuple_lst['g12'][7]/total_g12, tuple_lst['g13'][7]/total_g13]]\n",
    "\n",
    "    # # Define the pinkish color palette\n",
    "    colors = [\n",
    "        '#FFC0CB',  # Soft Pink\n",
    "        '#FFB6C1',  # Light Pink\n",
    "        '#FF69B4',  # Pink\n",
    "        '#FF1493',  # Hot Pink\n",
    "        '#FF3385',  # Deep Pink\n",
    "        '#C71585',  # Medium Violet Red\n",
    "        '#DB7093',  # Pale Violet Red\n",
    "        '#DA70D6'   # Orchid\n",
    "    ]\n",
    "\n",
    "    #THE C9P4 MUTANT GRAPH\n",
    "    # Initialize bottom heights\n",
    "    bottoms = [0] * len(groups)\n",
    "\n",
    "    # Plotting the stacked bar graph\n",
    "    for i in range(len(stacked)):\n",
    "        plt.bar(groups, stacked[i], bottom=bottoms, color=colors[i], label=f'Segment {i+1}', edgecolor = 'black', linewidth = 1.4)\n",
    "        # Update bottoms to include the height of the newly plotted segment\n",
    "        bottoms = [b + s for b, s in zip(bottoms, stacked[i])]\n",
    "\n",
    "    # Adding labels and title\n",
    "    plt.xlabel('Guide')\n",
    "    plt.ylabel('Percent of Reads')\n",
    "    plt.title(title)\n",
    "\n",
    "    # Adding a legend\n",
    "    plt.legend(categories,\n",
    "              loc = 'upper right',\n",
    "              frameon=False,\n",
    "              bbox_to_anchor = (1.8,1))\n",
    "\n",
    "    # Display the graph\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print(c9p4_mutant)\n",
    "\n",
    "make_bar_chart(c9p4_mutant, 'c9p4_mutant')\n",
    "make_bar_chart(c9p4_wt, 'c9p4_wildtype')\n",
    "\n",
    "make_bar_chart(c9p8_mutant, 'c9p8_mutant')\n",
    "print(c9p8_wt)\n",
    "make_bar_chart(c9p8_wt, 'c9p8_wildtype')\n",
    "\n",
    "make_bar_chart(c9p5_mutant, 'c9p5_mutant')\n",
    "make_bar_chart(c9p5_wt, 'c9p5_wildtype')\n",
    "\n",
    "\n",
    "#TODO (FROM THE NO DELETIONS EXCLUDE THOSE REVERSIONS - NEED TO SIFT THROGUH INDEPENDENTLY)\n",
    "c9p4_reversions = []\n",
    "c9p5_reversions = []\n",
    "c9p8_reversions = []\n",
    "\n",
    "c9p4_shifts = []\n",
    "c9p5_shifts = []\n",
    "c9p8_shifts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4139f0e2-bbcc-43d0-9857-24e4c1c7dc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if input a known interval of a heterozygous variant - will return the nucleotide sequence in that variant\n",
    "#thereby allowing verification of the phasing (for instance if split a BAM into WT and MUT, and have a known snp present on one allele at a \n",
    "#specified interval\n",
    "def find_interval(inbam, pos1, pos2):\n",
    "    deletions = []\n",
    "    count = 0\n",
    "\n",
    "    for read in inbam.fetch(\"chr9\", 27557179, 27580459):\n",
    "        #need to take into account soft clipped bases\n",
    "\n",
    "        count += 1\n",
    "        ref_start = read.reference_start  # Correct attribute for reference start position\n",
    "        if read.cigartuples[0][0] == 4:\n",
    "            ref_start = ref_start - read.cigartuples[0][1]\n",
    "\n",
    "        cigar = read.cigartuples\n",
    "\n",
    "        #the bases traversed - where we are in the read\n",
    "        l_read = 0\n",
    "        #the exact start position of the read - where we are in the reference\n",
    "        l = ref_start\n",
    "\n",
    "        segment_index = 0\n",
    "        for op, length in cigar:\n",
    "            #if we are at the exonic region in the read\n",
    "            if l + length > pos1:\n",
    "                deletions.append(read.query_sequence[l_read + (pos1 - l):l_read + (pos2 - l)])\n",
    "                break\n",
    "                \n",
    "            if op == 0:\n",
    "                l_read += length\n",
    "                l += length\n",
    "\n",
    "            elif op == 1:  # Insertion (this is correct)\n",
    "                l_read += length\n",
    "\n",
    "            elif op == 2:  # Deletion (this is correct)\n",
    "                l += length\n",
    "\n",
    "            elif op == 3:  # Skipped region\n",
    "                l += length #moving ahead in the reference\n",
    "\n",
    "            elif op == 4:  # Soft clipping\n",
    "                l_read += length\n",
    "                l += length #\n",
    "            elif op == 5:  # Hard clipping NOT SURE\n",
    "                pass\n",
    "            elif op == 6:  # Padding NOT SURE\n",
    "                pass\n",
    "            elif op == 7:  # Segment equal (this is correct)\n",
    "                l_read += length\n",
    "                l += length\n",
    "            elif op == 8:  # seems to be sequence mismatcj - they both advance then NOT SURE\n",
    "                l_read += length\n",
    "                l += length\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown CIGAR operation code {op}\")\n",
    "            segment_index += 1\n",
    "    return deletions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934d493f-a714-451c-a0f1-8a1e21d3828b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#verifuing the phasing for wt and mutant\n",
    "c9p8_g13_wt = pysam.AlignmentFile(\"c9p8_samples_merged/WT_Biosample_12.fofn.pbmm2.repeats.bam%.bam.merged.bam\", \"rb\" )\n",
    "c9p8_g13_mutant = pysam.AlignmentFile(\"c9p8_samples_merged/WT_Biosample_12.fofn.pbmm2.repeats.bam%.bam.merged.bam\", \"rb\" )\n",
    "result = find_interval(c9p8_g13_wt, 27574803, 27574806)\n",
    "result = find_interval(c9p8_g13_mutant, 27574804, 27574806)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342f253c-e7fe-4731-aa1f-5487ee27588a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#another categorization method \n",
    "#many of the indels are actually sequencing errors\n",
    "#settled on categorizing into desired excisions, undesired excisions (exonic overlap + intronic), inversions and a neither category\n",
    "#table to be used in PRISM plotting\n",
    "corse_tbl = pd.DataFrame()\n",
    "for index, row in read_table.iterrows():\n",
    "    # inversion\n",
    "    new_rows = []\n",
    "    if row['inversion']:\n",
    "        new_row = {\n",
    "            'Group': row['group'],\n",
    "            'Name': row['name'],\n",
    "            'Desired excision': 0,\n",
    "            'Undesired excision type': 0,\n",
    "            'Inversion': 1,\n",
    "            'No Excision or Inversion': 0\n",
    "        }\n",
    "    # undesired excision type I (overlaps with an exon)\n",
    "    elif row['excision which overlaps with ex1'] or row['excision which overlaps with ex2'] or row['excision which overlaps with ex1 and ex2']:\n",
    "        new_row = {\n",
    "            'Group': row['group'],\n",
    "            'Name': row['name'],\n",
    "            'Desired excision': 0,\n",
    "            'Undesired excision type': 1,\n",
    "            'Inversion': 0,\n",
    "            'No Excision or Inversion': 0\n",
    "        }\n",
    "    elif row['Intronic Excision?']:\n",
    "        new_row = {\n",
    "            'Group': row['group'],\n",
    "            'Name': row['name'],\n",
    "            'Desired excision': 1,\n",
    "            'Undesired excision type': 0,\n",
    "            'Inversion': 0,\n",
    "            'No Excision or Inversion': 0\n",
    "        }\n",
    "    else:  # Changed from elif: to else:\n",
    "        new_row = {\n",
    "            'Group': row['group'],\n",
    "            'Name': row['name'],\n",
    "            'Desired excision': 0,\n",
    "            'Undesired excision type': 0,\n",
    "            'Inversion': 0,\n",
    "            'No Excision or Inversion': 1\n",
    "        }\n",
    "\n",
    "    new_row_df = pd.DataFrame([new_row])\n",
    "    corse_tbl = pd.concat([corse_tbl, new_row_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68e98ad-7d6b-480e-9506-65503fefc615",
   "metadata": {},
   "outputs": [],
   "source": [
    "corse_tbl.groupby('Group').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9365bd5d-e14e-4e35-a90d-b0254e7e214c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mut_rows = corse_tbl[corse_tbl['Group'].str.contains('mut', case=False, na=False)]\n",
    "mut_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e494644b-6657-4022-993d-10c9e6ac77d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "wt_rows = corse_tbl[corse_tbl['Group'].str.contains('wt', case=False, na=False)]\n",
    "wt_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6679c74-613a-4134-a376-b14f7353bb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "c9p8_al = pysam.AlignmentFile('c9p8_samples_merged/MUTANT_Biosample_9.fofn.pbmm2.repeats.bam%.bam.merged.bam', \"rb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7215dcbf-76b6-46d2-b297-8fbd4162e900",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#summation over reads - for use in final table\n",
    "grouped = mut_rows.groupby('Group').sum()\n",
    "n = 3  # For example, extract the last 3 characters\n",
    "grouped['GroupLastChars'] = grouped.index.str[-n:]  # Extract last 'n' chars from the group names\n",
    "grouped_sorted = grouped.sort_values(by='GroupLastChars')\n",
    "grouped_sorted = grouped_sorted.drop(columns=['GroupLastChars'])\n",
    "display(grouped_sorted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8d4e52-1d9b-4046-af58-35e1461de9a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
